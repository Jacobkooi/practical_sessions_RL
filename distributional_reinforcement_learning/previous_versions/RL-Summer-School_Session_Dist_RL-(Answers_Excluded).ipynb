{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "executionInfo": {
     "elapsed": 5214,
     "status": "error",
     "timestamp": 1655991959184,
     "user": {
      "displayName": "Jesse van Remmerden",
      "userId": "16560922049388817820"
     },
     "user_tz": -120
    },
    "id": "PxKeKXB5z54P",
    "outputId": "671247fa-f72b-4a8c-d14f-c3dbd9d60b82"
   },
   "outputs": [],
   "source": [
    "# import necessary libraries \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import gym\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from random import shuffle\n",
    "from collections import deque\n",
    "from variables import *\n",
    "\n",
    "from IPython.display import YouTubeVideo\n",
    "from dist_dqn import dist_dqn\n",
    "from auxiliary import preproc_state, get_dist_plot\n",
    "from trainer import trainer, lossfn, get_action_dqn, get_target_dist\n",
    "\n",
    "import imageio\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NCKeR_4z54U"
   },
   "source": [
    "# Distributional Reinforcement Learning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQJNo1s7z54V"
   },
   "source": [
    "## The Expectation Revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLnGaoBHz54V"
   },
   "source": [
    "We will very briefly discuss how to compute the expectation. Recall that the expectation is essentially the weighted average of each of the values in the sample space. Formally\n",
    "\n",
    "$$\\large \\mathbb{E}[X] = \\sum_{i=1}^{n}x_{i}p_{i},$$\n",
    "\n",
    "where $\\mathbb{E}[X]$ denotes the expected value of a random variable $X$, $x_{i}$ a given outcome and $p_{i}$ the probability of that outcome. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URPifP0rz54W"
   },
   "source": [
    "In python, we can compute it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nREmxvF9z54W",
    "outputId": "e306d815-216f-43a7-f4f4-3205119e9337"
   },
   "outputs": [],
   "source": [
    "probs = np.array([0.6, 0.1, 0.1, 0.1, 0.1])\n",
    "outcomes = np.array([18, 21, 17, 17, 21])\n",
    "expected_value = 0.0\n",
    "\n",
    "for i in range(probs.shape[0]):\n",
    "    expected_value += probs[i] * outcomes[i]\n",
    "\n",
    "print(expected_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kEKvSwEz54W"
   },
   "source": [
    "A more succinct way, however, is to compute the expected value as the dot product of the possible outcomes and their corresponding probabilities. We can use the ``@`` operator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKNEgj6qz54X",
    "outputId": "eba4bfc5-68b0-468e-f44f-32aa3aaf6cca"
   },
   "outputs": [],
   "source": [
    "expected_value = probs @ outcomes\n",
    "print(expected_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBnz2CsDz54X"
   },
   "source": [
    "In distributional RL, instead of talking about the sample space (which can be infinite), we make use of the set of *possible* outcomes, called the **support**. The difference seems minor, but it is important to mention. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NoKRFIX-z54Y"
   },
   "source": [
    "## The Bellman Equation Revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tclZl9Wkz54Y"
   },
   "source": [
    "As we have seen throughout the week, most reinforcement learning algorithms determine the quality of an action through the *Bellman* equation:\n",
    "\n",
    "$$\\large Q(s_{t},a_{t}) \\leftarrow \\mathbb{E}[R(s_{t},a)] + \\gamma \\mathbb{E}[Q(S_{t+1}, A_{t+1})],$$\n",
    "\n",
    "where $Q(s_{t},a_{t})$ denotes the quality of the action given the current state, $\\gamma$ the discount factor and $Q(S_{t+1}, A_{t+1})$ the quality of the state and action at the next time step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRg5RtLKz54Y"
   },
   "source": [
    "## The *Distributional* Bellman Equation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNbY3Q7Iz54Y"
   },
   "source": [
    "A key difference between \"regular\" reinforcement learning and distributional reinforcement learning (DRL) is that in DRL, we do not \"collapse\" (which we call a *degenerate* distribution) into a single value, the expectation. \n",
    "\n",
    "As such, in distributional RL, we are interested in learning the *entire* value distributions. We can formalise it as follows:\n",
    "\n",
    "$$\\large Z(s_{t},a_{t}) \\leftarrow R(s_{t},a) + \\gamma Z(S_{t+1}, A_{t+1}),$$\n",
    "\n",
    "where $Z$ takes the place of $Q$. Note that it is just notational convention to use $Z$ instead of $Q$. We use $Z$ to make clear that we are talking about a *distribution* rather than a single Q-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWfXTEYcz54Z"
   },
   "source": [
    "## Updating the Distributional Bellman Equation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPy8bOL9z54Z"
   },
   "source": [
    "An essential part of distributional reinforcement learning deals with *updating* the distributional Bellman equation. In this section, we will discuss and implement an updating procedure for $Z$. \n",
    "\n",
    "Important to note here is that we will use Bayesian updating, meaning that we update a prior based on information we encounter from the environment (i.e. the rewards we get)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4SsHBaez54Z"
   },
   "source": [
    "To make things a bit more concrete, we will implement an updating procedure with a specific setting in mind. Here we will start with the game of Freeway. Freeway is an Atari game where the goal is for a chicken to cross  a highway without being hit by a car. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZRwtRc3z54a"
   },
   "source": [
    "The reward structure is as follows:\n",
    "\n",
    "- We received +10 reward if the chicken successfully crosses the highway.\n",
    "- If the game is lost (the chicken does not manage to cross the street within a certain amount of time), we get a reward of -10. \n",
    "- At all non-terminal steps, we get a reward of -1. We do this to make sure that the agent *acts* and does not wait for too long. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8K6dst1Az54a"
   },
   "source": [
    "Moreover, we have 3 possible actions:\n",
    "\n",
    "- NOOP. No operation, do nothing.\n",
    "- DOWN. Go down.\n",
    "- UP. Go up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OHJhuJgz54a"
   },
   "source": [
    "By means of a short illustration, consider the following clip:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Y23Urr_z54a"
   },
   "source": [
    "<img src=\"img/freeway.gif\" width=\"750\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wZ2EKWYz54b"
   },
   "source": [
    "### Prior Distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKdZsqJzz54b"
   },
   "source": [
    "We update the distribution in accordance with a Bayesian updating rule. In other words, we start with a prior and gradually update it to a posterior (target) distribution. The question is how to do this. Here we will follow the approach of Bellemare et.al. (2017) for approximate distributional learning which they formulate in terms of the **Categorical Algorithm**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cw9nGcYqz54b"
   },
   "source": [
    "First of all, it is important to cut off the reward distribution. If we would not do this, we would end up with an infinite range. As such, our distribution will be parameterized by two values, $V_{\\text{MIN}}$ and $V_{\\text{MAX}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSt-k9zEz54b"
   },
   "source": [
    "Since the most negative reward we can receive in Freeway is -10 and the most positive reward is +10, we will fix these two values as the limits of the reward distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epW4NC6Cz54b"
   },
   "outputs": [],
   "source": [
    "vmin,vmax = -10.,10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2wCMUe8z54b"
   },
   "source": [
    "Another key concept is the support. The support is the set of all possible outcomes we can obtain. In our case, this can be -10, +10, but also -1. Because of the probabilistic nature, however, we may also obtain reward values of, say, -4.2 or 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_osFlgBWz54c"
   },
   "source": [
    "For computational purposes, we work with *discrete* distributions. As such, all reward values need to be *binned* before we can actually work with them. Here we choose a prior distribution of 51 possible outcomes. These 51 values are the **support** of our distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WiRBAC0wz54c",
    "outputId": "aea442ef-bb09-47f9-dae5-a37b5f4bce37"
   },
   "outputs": [],
   "source": [
    "nsup=51  # number of items in the support \n",
    "support = np.linspace(vmin,vmax,nsup) \n",
    "print(support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04HK7cNTz54c"
   },
   "source": [
    "**Quick sidenote:** We could have chosen a different support number. Here we selected 51, because the difference between the bins is 0.4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VA6YYo0yz54c"
   },
   "source": [
    "Let us now construct the actual prior distribution. Note that because we do not know anything about the reward structure of the environment, we assume that each outcome is equally likely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0noQ0q4Dz54c",
    "outputId": "32af3774-419f-40e1-8707-f13214c30b12"
   },
   "outputs": [],
   "source": [
    "probabilities = np.ones(nsup)\n",
    "prior = probabilities / probabilities.sum()\n",
    "prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-KFTCV0z54d"
   },
   "source": [
    "We can visualize our distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_HHJIfGz54d",
    "outputId": "2e80e851-578b-45c2-fd0a-7ed777d7c0ad"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (6,5)\n",
    "plt.bar(support,prior, color = 'green')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Prior Reward Distribution')\n",
    "plt.xlabel('Rewards');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siDngEWnz54d"
   },
   "source": [
    "Note that we have two vectors: the ``support`` vector (which corresponds to all events with non-zero probability) and the ``probabilities`` vector which holds the probabilities for the values in the support vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7JMF6J_z54d"
   },
   "source": [
    "### Posterior Distribution: Locating the Reward Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKYbcVc4z54d"
   },
   "source": [
    "We will now gradually implement a function that does this. The goal of the function is to iteratively update the prior distribution in order to obtain a posterior distribution, $Z$. \n",
    "\n",
    "To reiterate, the Q-function we want to update is\n",
    "\n",
    "$$\\large Z(s_{t},a_{t}) \\leftarrow R(s_{t},a) + \\gamma Z(S_{t+1}, A_{t+1}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "endxjV2_z54d"
   },
   "source": [
    "Let us first set the parameters of the model. Here assume a $\\gamma$ of 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3w3uPiK3z54e"
   },
   "outputs": [],
   "source": [
    "gamma = 0.8\n",
    "Z = prior    # rename prior to Z\n",
    "observed_reward = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjuGJIZXz54e"
   },
   "source": [
    "Suppose that we are beginning to update the value distribution for our action $a_{\\text{up}}$. Also, assume that we have a reward of $5$.\n",
    "\n",
    "In that case, our belief is that $5$ is representative of the actual reward structure in the environment is being *reinforced*. This means that all other values are *less likely* and that observing a $5$ is *more likely*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zB4grzbCz54e"
   },
   "source": [
    "A key insight is that we *move probability mass from the other points to the centre of mass* (in this case 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmhKw7K0z54e",
    "outputId": "6fe8379f-f9aa-455c-e82f-808396192fc3"
   },
   "outputs": [],
   "source": [
    "# Assume that for a given action, we get a reward of 5\n",
    "plt.bar(support,Z, color = 'green')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Prior Reward Distribution')\n",
    "plt.xlabel('Rewards');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7wWHF3Uvz54e"
   },
   "source": [
    "The question is now *how* to update this particular distribution. More specifically, how do we *algorithmically* update the prior. Even more concrete - *where* do put that value on the support vector? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61N2Kn36z54e",
    "outputId": "81503e3b-7304-43f8-eb47-37837501beab"
   },
   "outputs": [],
   "source": [
    "print(support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJF4HnH5z54f"
   },
   "source": [
    "First of all, we observed that the spacing between the support values is 0.4. We do not need to observe this, but can also derive it using the following formula, which computes the closest support element:\n",
    "\n",
    "$$\\large \\Delta z = \\frac{V_{MAX} - V_{MIN}}{N-1}, $$\n",
    "\n",
    "where $\\Delta z $ denotes the closeness between the elements in the support vector and $N$ the number of elements in the support vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TG_iS2WCz54f"
   },
   "source": [
    "**Question 1**:  Verify that the space between each element is indeed 0.4 by implementing the equation above. Make sure that the variable name is ``dz``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHP1tmoIz54f",
    "outputId": "7662db85-6a3c-4398-f4c1-9164ec05b637"
   },
   "outputs": [],
   "source": [
    "# d(elta)z is the closest support element \n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7j5nW3CAz54f"
   },
   "source": [
    "As a next step, we want to pinpoint exactly where the value $5$ falls on the support vector. It is highly likely that the observed reward will not fall exactly on one of the elements in the support vector. As such, we need to find the *index* value of the *closest* support element, which we will call $b_{j}$.\n",
    "\n",
    "We can use the following formula to find the closest support element:\n",
    "\n",
    "$$\\large b_{j} = \\frac{r-V_{MIN}}{\\Delta z},$$\n",
    "\n",
    "where $r$ denotes the observed reward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EuMRb6CPz54f"
   },
   "source": [
    "Note that $b_{j}$ can take on any value in the support vector. In this case, $b_{j} \\in [0, N - 1].$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjX4K4mJz54g"
   },
   "source": [
    "**Question 2:** Implement the equation above. Make sure that $b_{i} \\in \\mathbb{Z}$ (i.e. make sure that $b_{i}$ is an integer value). Make sure to name the variable ``bj``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ap6y7r2z54g"
   },
   "outputs": [],
   "source": [
    "# compute bj, the index value of the support vector \n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuroYo_2z54g"
   },
   "source": [
    "Evaluate the value of ``bj``. Does it correspond to the location of the reward ``5`` in the plots above? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gzdgz5Wmz54g",
    "outputId": "a0f93cc4-1268-49ca-cf56-873334e9014b"
   },
   "outputs": [],
   "source": [
    "print(bj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NF4rGkfOz54g"
   },
   "source": [
    "### Posterior Distribution: Updating the Probability Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxWl5Ptqz54g"
   },
   "source": [
    "Because it is convenient to keep a copy of the original distribution, create a copy of the original ``probabilities`` vector and call it ``posterior``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uts5tBLkz54h",
    "outputId": "9c339fbb-000c-48f6-85e4-0aa81d01efad"
   },
   "outputs": [],
   "source": [
    "posterior = prior.copy()\n",
    "posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGr6f0Idz54h"
   },
   "source": [
    "Every time the agent encounters a reward we update the posterior distribution. One way to think of this is that probability mass is shifted from the left and right sides of the reward value to the value of the observed reward. In other words, probability mass gets *redistributed* towards the centre of mass. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-EbIV8ez54h"
   },
   "source": [
    "As illustrated below, our goal here is to update the uniform distribution by gradually moving mass to the centre of mass, both from the left side and from the right side. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRg2hTy-z54h"
   },
   "source": [
    "![alt text](img/normal_uniform.jpg \"Title\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02dYkwBUz54h"
   },
   "source": [
    "By now it should become clear why computing the index of the support vector was so important: we need it to determine *where* on the probabilities vector to move the mass to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_l4jVdHz54i"
   },
   "source": [
    "As such, the support act as our sample space and the probabilities vector to the probabilities on that sample space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzXQTw1Wz54i"
   },
   "source": [
    "In the context of reinforcement learning, **updating the target distributions** refers to the agent making an update to the empirical reward distribution each time it receives a reward. One way to think of this is that probability mass is shifted from the left and right sides of the reward value to the value of the observed reward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJoeVFOCz54i"
   },
   "source": [
    "Now, for each point in the probabilities vector (remember, there are 51 items in that vector), we gradually want to update the values by stealing some mass from its immediate neighbor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iupIeG6z54i"
   },
   "source": [
    "To make things a bit more precise, we will follow a two-step procedure:\n",
    "\n",
    "1. We will steal mass from the neighbors left of $b_{j}$.\n",
    "2. We will steal mass from the neighbors right of $b_{j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8AubN5uz54i"
   },
   "source": [
    "Let us consider a couple of cases of \"mass-stealing\" for the neighbors **left** of $b_{j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lrVjtNtz54i"
   },
   "source": [
    "$$ \\large m^{i}_{l} \\leftarrow m^{i}_{l} + \\gamma^{j} m^{i-1}_{l},$$\n",
    "\n",
    "where $m_{l}^{i}$ denotes the probability mass of the value we want to update, $\\gamma^{j} m^{i-1}_{l}$ how much we steal from its neighbor on the left and $\\gamma^{j}$ denotes the discount factor with an index $j$ (this tells us how far we are removed from the centre of mass - the further away from $b_{j}$ the fewer mass is being stolen).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBlwA28xz54i"
   },
   "source": [
    "**Question 3:** Consider for instance the following loop and observe how the value changes for different values of ``gamma``. How does the output changes for ``gamma = 1``, ``gamma = 0.8`` and ``gamma = 0.4``? And what happens as ``j`` increases? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjStY55Wz54j",
    "outputId": "267c6554-238c-4a14-9d22-044275461ab5"
   },
   "outputs": [],
   "source": [
    "gamma = 0.4\n",
    "\n",
    "for j in range(1, 5):\n",
    "    print(f'j: {j}. Value for second term: {np.round(np.power(gamma, j), 2)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWicsiqjz54j"
   },
   "source": [
    "The values around the observed reward value $r$ are more likely than values further down the line (e.g. if we observe 5, then 4 is more likely to be observed than -10). As such, we want to update each of the values in the probabilities (i.e. posterior) vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cK8bQs7Az54j"
   },
   "source": [
    "Important to note here is that *each* value of the distributions is being updated. As can be seen in the formula, the update always contain the value of $m^{i}_{l}$ itself plus a discounted update term, $\\gamma^{j} m^{i-1}_{l}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrIJKJv7z54j"
   },
   "source": [
    "Let us look at a couple of examples. First, let us look at what an update of the value for $r=5$, which, as we computed, is located at $b_{j} = 38$, would look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lyBbIzzRz54j",
    "outputId": "888b83b1-1ac5-45a7-fc3a-cd28f8f7b35b"
   },
   "outputs": [],
   "source": [
    "gamma = 0.8\n",
    "j = 1       # j indicates how far we are removed from the centre of mass bj\n",
    "updated_val = posterior[38] + ((gamma**j) * posterior[38-1])\n",
    "print(updated_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZlEo2cvCz54j"
   },
   "source": [
    "As can be seen, the new *probability value* at index *38* is now ~0.035 instead of ~0.020. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TToZSfVXz54k"
   },
   "source": [
    "Let us consider the next value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hzDsxXySz54k",
    "outputId": "40d752b1-e357-4df0-d5aa-20cdc7ba5fc4"
   },
   "outputs": [],
   "source": [
    "j = 2       # j indicates how far we are removed from the centre of mass bj\n",
    "updated_val = posterior[37] + ((gamma**j) * posterior[37-1])\n",
    "print(updated_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTgTAdsyz54k"
   },
   "source": [
    "Note that the new value is ~0.032. As can be seen, the index parameter *j* reduces the amount of mass shifted from one point to the next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HivxkCgbz54k"
   },
   "source": [
    "**Question 4:** Now it is your turn. Write a for loop that gradually updates each of the values on the left side of the centre of mass at $b_{j}$. Make sure to update the values in the ``posterior`` vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJwEGwGIz54k"
   },
   "outputs": [],
   "source": [
    "# redistribute probability mass for each of the values on the left side to the observed reward value\n",
    "# YOUR CODE HERE\n",
    "posterior = prior.copy()\n",
    "j = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsHxK6arz54k"
   },
   "source": [
    "Verify the output. You should observe a step-pattern on the left side of $b_{j} = 38$. As you can imagine, we also want to update the points on the right of the centre of mass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwVxzuYnz54l",
    "outputId": "8128e323-8fc7-4d66-8cb5-fd86d52fb751"
   },
   "outputs": [],
   "source": [
    "plt.bar(support,posterior, color = 'green')\n",
    "plt.title('Reward Distribution for an Action A')\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Rewards');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lafl7JLcz54l"
   },
   "source": [
    "**Question 5:** Update the points on the right side using the following update rule:\n",
    "\n",
    "$$ \\large m^{i}_{r} \\leftarrow m^{i}_{r} + \\gamma^{j} m^{r+1}_{r},$$\n",
    "\n",
    "where $m_{r}^{i}$ denotes any point on the right of $b_{j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7GPPx-Htz54l"
   },
   "outputs": [],
   "source": [
    "# redistribute probability mass for each of the values on the left side to the observed reward value\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fnpyf-YVz54l",
    "outputId": "2c46de07-fc2c-4959-8074-aa23ea91cc22"
   },
   "outputs": [],
   "source": [
    "plt.bar(support,posterior, color = 'green')\n",
    "plt.title('Reward Distribution for an Action A')\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Rewards');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2vLYjlGz54l"
   },
   "source": [
    "Because we are dealing with a probability distribution, check if the sum still equals 1. If this is not the case, normalize the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9OWmwwinz54m",
    "outputId": "90f457c4-a5dc-42a9-ba4f-044dbb495ab3"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "posterior /= posterior.sum() \n",
    "sum(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQSaAu86z54m"
   },
   "source": [
    "Run the code below. You should see a mode around the reward value of $5$, with a step pattern on both sides of this value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8Io4qtNz54m",
    "outputId": "c49f2c8f-4ab6-4faf-e9a7-4ae5afc8bf34"
   },
   "outputs": [],
   "source": [
    "plt.bar(support,posterior, color = 'green')\n",
    "plt.title('Updated Normalised Value Distribution for an Acion A')\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Rewards');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoR_XOuRz54m"
   },
   "source": [
    "We have now created all the necessary building blocks to build an updater function. \n",
    "\n",
    "**Question 6:** Combine all the individual pieces of code into a function that returns the updated posterior. Please do not change the input parameters - we will need this function later on. Moreover, make sure that the function returns the *entire* updated distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfzzYdy2z54m"
   },
   "outputs": [],
   "source": [
    "def update_dist(observed_reward, support, probabilities, vmin, vmax, gamma=0.8):\n",
    "    \"\"\"Function that updates the input distribution in accordance with observed reward.\n",
    "\n",
    "    Args:\n",
    "        observed_reward (int): Reward observed after taking some action.\n",
    "        support (torch.Tensor): Support vector.\n",
    "        probabilities (torch.Tensor): Probability vector. \n",
    "        vmin (float): Lower bound of the support.\n",
    "        vmax (float): Upper bound of the support.\n",
    "        gamma (float): Discount factor. \n",
    "\n",
    "    Returns:\n",
    "        posterior.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDhzmEdNz54n"
   },
   "source": [
    "Run the code below and evaluate the output. Does it cluster around the observed reward value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0vjMOri8z54n",
    "outputId": "14edbbd2-dce9-4df4-a4c9-c85bf1acbca1"
   },
   "outputs": [],
   "source": [
    "observed_reward = 5    # set hypothetical reward\n",
    "number_of_obs = 10     # set the number of observations\n",
    "Z = prior              # initialise prior\n",
    "\n",
    "for i in range(number_of_obs):\n",
    "    Z = update_dist(observed_reward,support,Z,vmin,vmax,gamma=0.8)\n",
    "\n",
    "plt.bar(support,Z, color = 'green')\n",
    "plt.title('Reward Distribution for an Action A')\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Rewards');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZE5mKobz54n"
   },
   "source": [
    "Note that the more often a certain reward is received by the agent, the higher the tendency to cluster around one value (or values, as we will see in a bit). As such, the variance decreases with more observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jeh_YZCz54n"
   },
   "source": [
    "Consider the code below. We now have a more complicated reward structure. Does it cluster around the various modes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l20F7LX9z54n",
    "outputId": "df58d4b6-1850-4ecd-9d0e-d72954a968df"
   },
   "outputs": [],
   "source": [
    "Z = prior\n",
    "ob_rewards = [10,10,10,0,1,0,-10,-10,10,10]\n",
    "for i in range(len(ob_rewards)):\n",
    "    Z = update_dist(ob_rewards[i], support, Z, vmin,vmax, gamma=0.7)\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.bar(support, Z, color = 'purple')\n",
    "plt.title('Distribution with Complex Reward Structure for an Action A')\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Rewards');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spnD0Fxoz54n"
   },
   "source": [
    "As can be seen, the reward structure is more complicated in this scenario. This can be hugely beneficial in the context of reinforcement learning because the agent can now make a more nuanced decision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kuHa28fnz54o"
   },
   "source": [
    "In this specific case, sometimes the action gives a reward of 0, most of the time a reward of 10, but a reward of -10 is also quite likely. This cannot be modelled by a \"classical reinforcement\" learner. For instance, if we just take the mean, we would think that this action yields a much lower reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8q9yUnTz54o",
    "outputId": "eefe0e01-e66d-4b91-e1be-eb628ddda515"
   },
   "outputs": [],
   "source": [
    "np.mean(ob_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDyEO6g-z54o"
   },
   "source": [
    "Furthermore, incorporated within this distribution is a risk-reward trade-off. Sometimes taking this action may yield +10, but it may sometimes also lead to a catastrophic failure (i.e. if we get -10). The \"classical\" agent may not infer this from a simple expected reward of 3.1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7dnsS1wz54o"
   },
   "source": [
    "## Selecting an Action: Collapse of the Reward Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0xbfm8Jz54o"
   },
   "source": [
    "We have been implementing an updating procedure for one single action. Let us now consider consider a hypothetical scenario where we play a game with 3 actions. The reward structures are defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8psJJ56z54o"
   },
   "outputs": [],
   "source": [
    "ob_rewards1 = [10,10,10,0,1,0,-10,-10,10,10]\n",
    "ob_rewards2 = [5,5,5,1,5,4,5,5,5,5]\n",
    "ob_rewards3 = [-4,-3,-4,0,-2,-4,-4,-3,-5,-5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcRXdqXBz54p"
   },
   "source": [
    "For each of the actions, we defined a prior. As before, our prior will be a simple, uniform distribution with a support of 51. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-AQ_q71uz54p",
    "outputId": "88222fe1-cd70-4191-ebdf-2a495c03c98f"
   },
   "outputs": [],
   "source": [
    "probabilities = np.ones(51)\n",
    "prior = probabilities / probabilities.sum()\n",
    "prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zimpih-Xz54p"
   },
   "source": [
    "We can now define three priors over the actions spaces, $Z_{1}$, $Z_{2}$ and $Z_{3}$ - three distributions we need to update. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MOm8VJrRz54p"
   },
   "outputs": [],
   "source": [
    "Z1, Z2, Z3 = prior, prior, prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKKxmhVVz54p"
   },
   "source": [
    "As before, we can loop over the update function. Now, however, we do this for each of the three actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mLc6N6DKz54p"
   },
   "outputs": [],
   "source": [
    "for i in range(len(ob_rewards1)):\n",
    "    Z1 = update_dist(ob_rewards1[i], support, Z1, vmin,vmax, gamma=0.7)\n",
    "    Z2 = update_dist(ob_rewards2[i], support, Z2, vmin,vmax, gamma=0.7)\n",
    "    Z3 = update_dist(ob_rewards3[i], support, Z3, vmin,vmax, gamma=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuP7pXDQz54q"
   },
   "source": [
    "Let us visualize three distributions for the 3 actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YX-lse7Jz54q",
    "outputId": "4b47c177-2267-4c5b-dac4-b9fc6d4e3621"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20,5)\n",
    "\n",
    "#The below code will create two plots. The parameters that .subplot take are (row, column, no. of plots).\n",
    "plt.subplot(1,3,1)\n",
    "action1 = plt.bar(support, Z1, color = 'red')\n",
    "plt.title(f'Action 1, E: {np.round(support @ np.array(Z1), 2)}')\n",
    "plt.subplot(1,3,2)\n",
    "action2 = plt.bar(support, Z2, color = 'green')\n",
    "plt.title(f'Action 2, E: {np.round(support @ np.array(Z2), 2)}')\n",
    "plt.subplot(1,3,3)\n",
    "action3 = plt.bar(support, Z3, color = 'purple')\n",
    "plt.title(f'Action 3, E: {np.round(support @ np.array(Z3), 2)}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRGIvJ9Az54q"
   },
   "source": [
    "As can be seen, we have 3 distributions. Each of these corresponds to the distribution of the reward structure. In other words, based on $n$ observations, taking the action $A_{1}$ yields $Z_{1}$, $A_{2}$ yields $Z_{2}$ and $A_{3}$ yields $Z_{3}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PLM7TeHz54q"
   },
   "source": [
    "Because we are working with distributions over the action space, let us combine $Z_{1}$, $Z_{2}$ and $Z_{3}$ into a 3X51 array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XdVjlGwJz54q",
    "outputId": "f9f797e4-59a2-4b70-b499-9ea4a87e6547"
   },
   "outputs": [],
   "source": [
    "dist = np.array([Z1, Z2, Z3])\n",
    "dist.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFixgkOuz54q"
   },
   "source": [
    "Selecting an action $a'$ from the set of possible actions on the next step can be modeled as follows:\n",
    "\n",
    "$$\\large \\underset{a' \\in A_{t+1}}{\\text{argmax}} \\mathbb{ E } [Z(S_{t+1})].$$\n",
    "\n",
    "Thus, we select the action that maximizes the expected value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D57p5VK3z54r"
   },
   "source": [
    "**Question 7:** Write a piece of code that selects, from these three actions, the best one. \n",
    "\n",
    "Hint: As discussed in the beginning, the expectation can be computed as the dot product of the support vector and its corresponding probabilities ``expected_value = probs @ outcomes``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTdNkUr1z54r"
   },
   "outputs": [],
   "source": [
    "def get_action(dist,support):\n",
    "    \n",
    "    \"\"\"This function returns an integer action in [0,1,2].\n",
    "    \n",
    "    Args: \n",
    "        dist (np.array): Input is a Ax51 array.\n",
    "        support (np.array): Support vector.\n",
    "\n",
    "    Returns:\n",
    "        actions (int): vector of integers in {0,1,2}, dimension dist.shape[0] \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lU7uslepz54r",
    "outputId": "1659a98b-09f6-4154-edc3-bbc4cb7a6b3d"
   },
   "outputs": [],
   "source": [
    "get_action(dist,support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTr2ma4Yz54r"
   },
   "source": [
    "As can be seen, based on the distributions, we should select action 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-v55r0M1z54r"
   },
   "source": [
    "## Distributional Deep Reinforcement Learning\n",
    "\n",
    "We will now implement an actual game, Atari Freeway. As said before, the goal of this game is the run across a highway and gain as many points as possible. If you get hit by a car, you are being pushed back. If you reach the other side, you will get a reward of 1. Watch a couple of minutes of the clip below for a visual demonstration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RERk15K-z54r"
   },
   "source": [
    "<img src=\"img/freeway.gif\" width=\"750\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TkPFKDbz54s"
   },
   "source": [
    "We will now have a look at a Distributional DQN implementation of the Atari Freeway Game. We will start with a couple of functions. Some are given, and some require you to complete a line of code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSP3hWK2z54s"
   },
   "source": [
    "To simulate the environment, we will make use of OpenAI ``gym``. Have a look at the actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "he9A0Ng_z54s",
    "outputId": "0488c5f9-3564-4c6b-8a40-0e04a475b7dd"
   },
   "outputs": [],
   "source": [
    "env = gym.make('Freeway-ram-v0') \n",
    "aspace = 3                          # action space size\n",
    "env.env.get_action_meanings()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gj04XWzz54s"
   },
   "source": [
    "Let us now set the other hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0rNvUMaz54s"
   },
   "outputs": [],
   "source": [
    "vmin,vmax = -10,10\n",
    "replay_size = 200\n",
    "batch_size = 50\n",
    "nsup = 51\n",
    "gamma = 0.1\n",
    "episodes = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2HLmMxOz54s"
   },
   "source": [
    "Let us now initialise the network parameters. Note that we did not include the network in this notebook. If you are interested you can check out the network in the ``dist_dqn.py``-file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngJsS-Zkz54t"
   },
   "source": [
    "**Question 8 (Bonus):** Run the trainer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wb1qMnF-z54t"
   },
   "source": [
    "We can now train the model by calling the trainer function. This function returns $\\theta$, ``theta``, the parameters of the neural network and the ``losses``, which are the losses over time. The exact workings of this function are beyond the scope of this session, but you are encouraged to check out the code yourself (to be found in the ``trainer.py`` file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgB5l9ibz54t",
    "outputId": "3a3e0cb9-1f60-4d86-edc8-3610419345d7"
   },
   "outputs": [],
   "source": [
    "# start training\n",
    "theta, losses = trainer(env, aspace, nsup, vmin, vmax, gamma, episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDrkA9JLz54t"
   },
   "source": [
    "Let us now visualize the loss over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-dLqQVdz54t",
    "outputId": "444d5092-2893-45ae-9e5a-6cb26a3f8d4a"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses) \n",
    "plt.title('Cross Entropy Loss over Time for the Atari Freeway Agent')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Episode');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOxGHqKGz54u"
   },
   "source": [
    "We can also observe how the agent is doing. Run the cell below to generate a visualization of the DQN agent playing a game of Atari Freeway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xC2GUTW_z54u",
    "outputId": "33188aa3-b376-490d-bfb0-266b6f8afcc5"
   },
   "outputs": [],
   "source": [
    "# create empty list, to be populated by frames\n",
    "clip = []\n",
    "\n",
    "# reset the game\n",
    "state = preproc_state(env.reset())\n",
    "for i in range(1000):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Rendering: {(i/1000)*100}% complete...\")\n",
    "    \n",
    "    # make a prediction for a given state\n",
    "    pred = dist_dqn(state,theta,3)\n",
    "    # get the image\n",
    "    image = get_dist_plot(env,pred,support,shape=(210,160,3))\n",
    "    #pred = pred.detach.numpy()\n",
    "    # select best action\n",
    "    action = get_action_dqn(pred.unsqueeze(dim=0).detach().numpy(),support).item()\n",
    "    # get state, reward and information\n",
    "    s,r,d,j = env.step(action)\n",
    "    # preprocess new state\n",
    "    state = preproc_state(s)\n",
    "    # append individual frames\n",
    "    clip.append(image)\n",
    "    \n",
    "print('Rendering complete! Converting to mp4...')\n",
    "\n",
    "# write to mp4\n",
    "imageio.mimwrite('img/AtariHighway-v3.mp4', clip, fps = 20)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pEfY-KFz54u"
   },
   "source": [
    "**Question 9 (Bonus):** Consider the clip below. Explain the behaviour of the agent using the distribution on the right-hand side of the game. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4acvpvbiz54u",
    "outputId": "0c6811a5-d5bc-4614-9d54-e17f4619be50"
   },
   "outputs": [],
   "source": [
    "<video width='600' height='500' controls>\n",
    "  <source src='img/AtariHighway-v3.mp4' type='video/mp4'>\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UX2hpRpRz54v"
   },
   "source": [
    "# References \n",
    "\n",
    "- [A Distributional Perspective on Reinforcement Learning, Bellemare et.al. (2017)](https://arxiv.org/pdf/1707.06887.pdf)\n",
    "- *Deep Reinforcement Learning in Action*, Zai & Brown 2020.\n",
    "- [Deep Reinforcement Learning in Action, Zai & Brown, Github Repository](https://github.com/DeepReinforcementLearning/DeepReinforcementLearningInAction)\n",
    "- [Distributional Reinforcement Learning, Bellemare, Dabney and Rowland (2022, draft)](https://www.distributional-rl.org)\n",
    "- [Distributional Reinforcement Learning with Quantile Regression, Dabney et.al. (2017)](https://arxiv.org/abs/1710.10044)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "RL-Summer-School_Session_Dist_RL-(Functions_Excluded).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
