{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec09daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e780f5",
   "metadata": {},
   "source": [
    "# Symmetries in RL - Practical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5f5084",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "Policy gradients (PG) allow for a great flexibility in the parameterization of policies to solve MDPs (See practical XXX earlier this week).\n",
    "\n",
    "Policy gradients algorithms rely on the PG theorem which expresses the gradient of the RL loss (here expected sum of discounted future loss $V_{\\theta} = \\mathbb{E}[\\sum_i^{\\infty} \\gamma^i r_i]$) in a form amenable to sample estimation.\n",
    "Noting $\\theta\\in \\Theta$ the paramaters of a class of policies $\\{\\pi_{\\theta} \\mid \\theta \\in \\Theta \\}$.\n",
    "\n",
    "$$\\nabla_\\theta \\, V_{\\theta} = \\mathbb{E}_{\\pi} [Q^\\pi(s,a) \\nabla_{\\theta} \\log\\,\\pi(a|s)]$$\n",
    "\n",
    "In PG algorithms, iterative updates to $\\theta$ are made by following (an estimate of) the negative gradient $g_{\\theta} = -\\nabla_\\theta \\, V_{\\theta}$ until convergence.\n",
    "\n",
    "$$\\theta \\leftarrow \\theta + \\alpha g_{\\theta}$$\n",
    "\n",
    "\n",
    "In particular, it can be used to encode prior information about the system into the policy.\n",
    "\n",
    "The aim of this notebook is to explore ways to introduce *symmetry* assumptions about the MDP into the policy.\n",
    "This notebook is based on the paper: [MDP Homomorphic Networks: Group Symmetries in Reinforcement Learning](https://arxiv.org/abs/2006.16908) by van der Pol et al."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532bb715",
   "metadata": {},
   "source": [
    "## Symmetries in RL: a brief intro\n",
    "\n",
    "Cartpole example\n",
    "* state $s = [x, \\dot{x}, \\theta, \\dot{\\theta}]$ (cart position, pole angle, derivatives)\n",
    "* action $a \\in \\{\\leftarrow, \\rightarrow\\}$, (lateral force on the cart)\n",
    "\n",
    "There is a symmetry around $s_c = [0,0,0,0]$.\n",
    "Consider \n",
    "* the reflexion operator $L[s] = -s$\n",
    "* the swap operator on a binary policy $\\pi=[p, 1-p]$:  $K[\\pi] = [1-p, p]$\n",
    "\n",
    "The optimal policy $\\pi^*$ can be shown to satisfy\n",
    "$$ K[\\pi^*(s)] = \\pi^*(L[s])$$\n",
    "\n",
    "In other words\n",
    "$$\\pi^*(\\leftarrow|s) = \\pi^*(\\rightarrow|-s) $$\n",
    "\n",
    "\n",
    "![title](mdp_hom.png)\n",
    "\n",
    "\n",
    "The consequence is you interactions on both sides of the point of symmetry can inform the same simpler policy $\\bar{\\pi}$ defined for  $s \\in (\\mathbb{R}^{+})^{4}$, and lead to better **sample efficiency**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a0db6c",
   "metadata": {},
   "source": [
    "## Understanding Invariance and Equivariance\n",
    "\n",
    "Let $G$ be a group indexing a set of transformations operators $L_g : X \\to X$, $g \\in G$ \n",
    "\n",
    "Let $f$ be a mapping from $X$ to $Y$\n",
    "\n",
    "### Invariance\n",
    "\n",
    "$f$ is invariant or symmetric to $L_g$ if $f(x) = f(L_g[x])$, for all $g \\in G$, $x \\in X$\n",
    "\n",
    "$\\{L_g\\}_{g\\in G}$ is a set of symmetries of $f$ \n",
    "\n",
    "For example, convolutional networks are invariant to translation of the input.\n",
    "\n",
    "\n",
    "\n",
    "### Equivariance\n",
    "\n",
    "$f$ is equivariant to $L_g$ if there exists a __second__ transformation operator $K_g : Y \\to Y$ in the output space of $f$ such that \n",
    "\n",
    "$\\quad K_g[f(x)] = f (L_g [x])$, for all $g \\in G, x \\in X$ .\n",
    "\n",
    "\n",
    "This is a good property to have in image segmentation models with respect to translations and rotations (with $K_g=L_g$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68252c53",
   "metadata": {},
   "source": [
    "## Identifying the Symmetries of an MPD\n",
    "\n",
    "**MDP with symmetries**. \n",
    "\n",
    "In an MDP with symmetries there is a set of transformations on the state-action space, which leaves the reward function and transition operator invariant. We define a state transformation and a state-dependent action transformation as $L_g : S \\to S$ and $K_g^s : A \\to A$ respectively. Invariance of the reward\n",
    "function and transition function is then characterized as\n",
    "\n",
    "$\\quad\\quad R(s, a) = R(L_g [s], K^s_g [a])$ for all $g \\in G, s \\in S, \\in  A$ \n",
    "\n",
    "$\\quad\\quad T (s′|s, a) = T (L_g [s′]|L_g [s], K^s_g [a])$ for all $g \\in G, s \\in S, a \\in A.$ \n",
    "\n",
    "\n",
    "For the cartpole example, there are 2 pairs of input / output transformations to that leave the optimal policy unchanged:\n",
    "- the identity / identity pair.\n",
    "- the reflexion around $s_c= (0,0,0,0)$ / swap of policy outcomes\n",
    "\n",
    "Let's code these up. Both of these can be written, respectively, as matrix operations on the state and policy output vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5975f331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cartpole_state_group_representations() -> List[torch.TensorType]:\n",
    "    \"\"\"\n",
    "    Matrix representation of the group symmetry on the state: \n",
    "    * identity\n",
    "    * a multiplication of all state variables by -1\n",
    "    \n",
    "    return: a list of two 4*4 matrices\n",
    "    \"\"\"\n",
    "    # FOR STUDENTS\n",
    "    # raise NotImplementedError\n",
    "    \n",
    "    # SOLUTION\n",
    "    return [torch.FloatTensor(np.eye(4)),\n",
    "            torch.FloatTensor(-1*np.eye(4))]\n",
    "\n",
    "def get_cartpole_action_group_representations() -> List[torch.TensorType]:\n",
    "    \"\"\"\n",
    "    Representation of the group symmetry on the policy: \n",
    "    * identity\n",
    "    * a permutation of the actions\n",
    "    \n",
    "    return: a list of two 2*2 matrices\n",
    "    \"\"\"\n",
    "    # FOR STUDENTS\n",
    "    # raise NotImplementedError\n",
    "    \n",
    "    # SOLUTION\n",
    "    return [torch.FloatTensor(np.eye(2)),\n",
    "            torch.FloatTensor(np.array([[0, 1], [1, 0]]))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10165c17",
   "metadata": {},
   "source": [
    "Let's test state and action group representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caab2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTHING TO DO: this cell just checks the function you coded work as intended.\n",
    "# More precisely it checks that transformations form a group and can be composed.\n",
    "\n",
    "state_group_reps = get_cartpole_state_group_representations()\n",
    "action_group_reps = get_cartpole_action_group_representations()\n",
    "\n",
    "# picks two indices\n",
    "i,j = np.random.randint(0, len(state_group_reps)-1, size=2)\n",
    "\n",
    "# check that composition of 2 elements in the group, stays in the group\n",
    "for group_rep in [state_group_reps, action_group_reps]:\n",
    "    new_element = torch.matmul(group_rep[i], group_rep[j])\n",
    "    assert any([torch.equal(new_element, g) for g in group_rep])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b402e60",
   "metadata": {},
   "source": [
    "## Building Equivariant Layers for a policy\n",
    "\n",
    "\n",
    "### Goal\n",
    "\n",
    "Having identified the pairs of transformations, we know the optimal policy will satisfy the equivariance property: \n",
    "\n",
    "$$ K_g[\\pi^*(s)] = \\pi^*(L_g[s]), \\forall g \\in G$$\n",
    "\n",
    "we can build a neural network layer satisfying this property with fewer parameters and better generalization properties than a network that does not make this assumption\n",
    "\n",
    "### Building an equivariant network\n",
    "\n",
    "Classic Neural network layer \n",
    "$$ z' = W z + b$$\n",
    "\n",
    "For a given pair of linear group transformation operators in matrix form $(L_g , K_g)$, where $L_g$ is the input transformation and $K_g$ is the output transformation, we then have to solve the equation\n",
    "\n",
    "$$K_g W z = W L_g z, \\forall g \\in G, z$$\n",
    "\n",
    "\n",
    "Space of Equivariant weights\n",
    "$$W_{eq} = \\{ W ∈ W_{total} | K_g W = W L_g , \\forall g \\in G\\}$$\n",
    "\n",
    "\n",
    "Symmetrizer of weights\n",
    "$$S(W) = \\frac{1}{|G|}\\sum_{g\\in G} K_g^{−1}  W L_g $$\n",
    "\n",
    "We have that\n",
    "$$\\forall W \\in W_{total}, S(W) \\in W_{eq}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b907d882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a pre-existing neural network layer into an equivariant layer \n",
    "# via 'symmetrization' given the identified invariances\n",
    "\n",
    "def symmetrize(W: torch.TensorType, group: List[torch.TensorType]) -> torch.TensorType:\n",
    "    \"\"\"\n",
    "    Create equivariant weight matrix\n",
    "    INPUT\n",
    "    :param W: input weight of size 2 x 4\n",
    "    :group: the invariance representation\n",
    "    OUTPUT\n",
    "    the symmetrized weight of size 2 x 4\n",
    "    \"\"\"\n",
    "    \n",
    "    # STUDENTS\n",
    "    # raise NotImplementedError\n",
    "    \n",
    "    # SOLUTION    \n",
    "    num_elements = len(group[0]) # number of transformations\n",
    "    W_sym = torch.zeros_like(W)\n",
    "    for g in range(num_elements):\n",
    "        W_sym += torch.matmul(group[1][g], torch.matmul(W, group[0][g]))\n",
    "    W_sym /= num_elements\n",
    "    return W_sym\n",
    "        \n",
    "        \n",
    "# Let's check shapes\n",
    "group = [get_cartpole_state_group_representations(),\n",
    "         get_cartpole_action_group_representations()]\n",
    "W = torch.tensor(np.random.rand(2, 4).astype(np.float32))\n",
    "W_sym = symmetrize(W, group)\n",
    "\n",
    "assert W.shape == W_sym.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b7ff00",
   "metadata": {},
   "source": [
    "Let's check the symmetrized weights do indeed parameterize a policy with the desired equivariance property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7983792",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# use the following helper function\n",
    "def test_network_is_equivariant(W, z, group):\n",
    "    \"\"\" testing for a specific input z \"\"\"\n",
    "    \n",
    "    is_equivariant = []\n",
    "    for i in range(len(group[0])):\n",
    "\n",
    "        Lz = torch.matmul(group[0][i], z)\n",
    "        WLz = torch.matmul(W, Lz)\n",
    "        Wz = torch.matmul(W, z)\n",
    "        KWz = torch.matmul(group[1][i], Wz)\n",
    "\n",
    "        is_equivariant.append(torch.equal(KWz, WLz))\n",
    "    assert all(is_equivariant)\n",
    "\n",
    "    \n",
    "# SOLUTION\n",
    "# get state / action groups\n",
    "group = [get_cartpole_state_group_representations(),\n",
    "         get_cartpole_action_group_representations()]\n",
    "\n",
    "# random state\n",
    "z = torch.tensor(np.random.rand(4,1).astype(np.float32))\n",
    "test_network_is_equivariant(W_sym, z, group)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53e2879",
   "metadata": {},
   "source": [
    "## Building a basis for Equivariant layers\n",
    "\n",
    "When working with discrete state actions, the set of equivariant weight\n",
    "$$W_{eq} = \\{ W ∈ W_{total} | K_g W = W L_g , \\forall g \\in G\\}$$\n",
    "\n",
    "is a linear subspace of the space of weights $W_{total}$.\n",
    "\n",
    "To parameterize an equivariant layer, it is enough to express the weights in a basis of $W_{eq}$\n",
    "\n",
    "This can be done by the following procedure\n",
    "* 1) sample non-equivariant weights  $(W_n)_{n=1..N}$\n",
    "* 2) symmetrize those weights $\\tilde{W}_n = S(W_n)$\n",
    "* 3) find a basis for $W_{eq}$ from $(\\tilde{W}_n)_{n=1..N}$, i.e. find $\\{V_i\\}_{i=1}^r$ such that $\\forall w \\in W_eq, \\exists c \\in \\mathbb{R}^r, w = \\sum_i c_i V_i$ (and use $c$ as a parameter vector)\n",
    "\n",
    "\n",
    "Let's do this for the cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5db5e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_equivariant_basis(state_group, action_group, size):\n",
    "    \"\"\"\n",
    "    Get equivariant basis by finding the subspace of symmetrized samples of (non-equivariant weights)\n",
    "    \n",
    "    \"\"\"\n",
    "    # STUDENTS\n",
    "    # raise NotImplementedError\n",
    "    \n",
    "    # SOLUTION\n",
    "    # sample multiple random weights\n",
    "    w = np.random.randn(*size)\n",
    "    w = torch.tensor(w.astype(np.float32))\n",
    "\n",
    "    # symmetrize\n",
    "    w = symmetrize(w, group)\n",
    "\n",
    "    # Vectorize W\n",
    "    wvec = np.reshape(w, [w.shape[0], -1])\n",
    "\n",
    "    # Get basis of symmetrized via SVD \n",
    "    __, s, vh = np.linalg.svd(wvec)\n",
    "    rank = np.linalg.matrix_rank(wvec)\n",
    "    new_size = [-1] + list(size[1:])\n",
    "    # Unvectorize W\n",
    "    w = np.reshape(vh[:rank, ...], newshape=new_size)\n",
    "    basis = w.astype(np.float32)\n",
    "    return basis, rank\n",
    "\n",
    "\n",
    "\n",
    "state_group_reps = get_cartpole_state_group_representations()\n",
    "action_group_reps = get_cartpole_action_group_representations()\n",
    "\n",
    "basis, rank = get_equivariant_basis(state_group_reps, action_group_reps, size=(50, 2, 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9755f2ff",
   "metadata": {},
   "source": [
    "Let's test this basis!\n",
    "Build some equivariant weights from this basis and test the resulting weights are indeed equivariant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80ef858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build some weights as a linear combination of the basis\n",
    "# weights = ... \n",
    "\n",
    "# SOLUTION\n",
    "c = np.random.rand(rank)\n",
    "weights = np.einsum('sij,s->ij', basis, c)\n",
    "\n",
    "\n",
    "\n",
    "# let's check if the associated network is indeed equivariant\n",
    "z = torch.tensor(np.random.rand(4,1).astype(np.float32))\n",
    "weights = torch.tensor(weights.astype(np.float32))\n",
    "\n",
    "test_network_is_equivariant(weights, z, group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd218d3",
   "metadata": {},
   "source": [
    "## Training an equivariant policy\n",
    "\n",
    "We now have all the tools to build an equivariant policy.\n",
    "\n",
    "Putting all these elements together is beyond the scope of this tutorial.\n",
    "\n",
    "Luckily, there is code online to do just this: \n",
    "* https://github.com/ElisevanderPol/mdp-homomorphic-networks\n",
    "* or https://github.com/ElisevanderPol/marl_homomorphic_networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b1eace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22b11ab2",
   "metadata": {},
   "source": [
    "## Compare with a non-equivariant policy\n",
    "\n",
    "By encoding some a priori structure into the policy class, the hope is to open a more sample-efficient policy gradient algorithm.\n",
    "\n",
    "Let's put this to the test and compare the dynamics of training via policy gradient for both equivariant and non-equivariant policy classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cca45a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
