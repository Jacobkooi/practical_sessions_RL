{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d0e355",
   "metadata": {},
   "source": [
    "# Reinforcement learning summer school at the VU - 2022 - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3496fe3e",
   "metadata": {},
   "source": [
    "## Workshop tutorial, day 2: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086ba19c",
   "metadata": {},
   "source": [
    "## Deep Reinforcement Learning Agent (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a72ffd",
   "metadata": {},
   "source": [
    "### Author: Buelent Uendes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcd5edf",
   "metadata": {},
   "source": [
    "Before we dive into deep reinforcement learning, it is a good idea to familiarize ourselves with the gym environment provided by OpenAI and start with the simple Q learning agent. In the second notebook (Deep Reinforcement Learning Agent (Part 2), we will show how we implement a deep reinforcement learning algorithm to solve the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d174629f",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "\n",
    "In the notebook, you will see a couple of ToDos with some instructions. Try your best to work through them and to complete the notebook. In case you run into problems, do not hesitate to ask any of the TAs for help! :) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d51520",
   "metadata": {},
   "source": [
    "## Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3833d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "\n",
    "#Set the seed for reproducibility\n",
    "random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc733553",
   "metadata": {},
   "source": [
    "## Open AI gym and the mountain car problem "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96559ce9",
   "metadata": {},
   "source": [
    "In the following, we will train our reinforcement learning algorithms on classical control problems provided by OpenAI gym. In particular, we will focus on the so-called mountain car problem. In this problem, the agent needs to learn how to drive a car up the right hill, given the contraint that the engine of the car is not strong enough to do right away. Starting from the bottom of the hill, the agent therefore needs to go back and forth, thereby creating enough momentum, to drive up the mountain. Compared to other classical control problems, this is a quite challenging environment, as the agent never receives a non-negative reward unless the agent reaches the right hill. Thus, most of the time the agent receives a negative reward of -1 which makes learning difficult.\n",
    "\n",
    "You can find more information about the mountain car environment [here](https://www.gymlibrary.ml/environments/classic_control/mountain_car/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5effbb09",
   "metadata": {},
   "source": [
    "## Quick recap of working with the gym environment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d09954c",
   "metadata": {},
   "source": [
    "## 1) Creating the environment \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912be3db",
   "metadata": {},
   "source": [
    "Creating an environment, is fairly easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1a56681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_name = 'MountainCar-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "#Set the seed\n",
    "env.action_space.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f11286",
   "metadata": {},
   "source": [
    "## 2) Getting familiar with some of the properties of the problem "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc883cd",
   "metadata": {},
   "source": [
    "Before starting to work on the problem, it is important to familiarize oneself with the problem at hand, i.e. investigating the observation space and the action space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b55f945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The observation space is  Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "The action space is  Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "print('The observation space is ', env.observation_space)\n",
    "print('The action space is ', env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648d7ff7",
   "metadata": {},
   "source": [
    "As we can see, the observation space is continous with discrete number of actions. The agent can choose from three different actions:\n",
    "\n",
    "- 0: Accelerate to the left\n",
    "- 1: Do not accelerate\n",
    "- 2: Accelerate to the right\n",
    "\n",
    "One way to deal with continous action space is to discretize the state space into bins. This simplifies the problem and is a prerequisite for any tabular approach such as the Q learning.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bea4d93",
   "metadata": {},
   "source": [
    "To get a better idea of the observation space, we can check the range of the observation space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c10ad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "low = env.observation_space.low\n",
    "high =env.observation_space.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db62df18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.2 , -0.07], dtype=float32), array([0.6 , 0.07], dtype=float32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low, high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5391a23",
   "metadata": {},
   "source": [
    "The first value of the observation space represents the x position, whereas the second one represents the velocity of the car. As we can see, the x position is in the range between $[-1.2, 0.6]$, whereas velocity lies in the range between $[-0.07, 0.07]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9122dfa9",
   "metadata": {},
   "source": [
    "To create a new starting position, one can use the reset command as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b9c24b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.41582763,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7807904d",
   "metadata": {},
   "source": [
    "We an sample a random action from the environment via the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d07d0087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.4166219 , -0.00079428], dtype=float32), -1.0, False, {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f21fecc",
   "metadata": {},
   "source": [
    "## First step: Random agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca638e67",
   "metadata": {},
   "source": [
    "It is always a good idea to test out the environment and start to play around. For this purpose, one can create an agent that plays the game with random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ccf19f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "env.reset()\n",
    "for _ in range(500):\n",
    "    if _ % 100 == 0:\n",
    "        print(_)\n",
    "    action = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "\n",
    "# This will stop the environment after\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204ecd9c",
   "metadata": {},
   "source": [
    "## In conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0b9403",
   "metadata": {},
   "source": [
    "As a recap, the main four functions for the environment space are:\n",
    "- **env.reset():** \n",
    "    Resets the environment and obtain initial starting observation\n",
    "- **env.render():** \n",
    "    Visualize the environment. Important Pygame needs to be installed for this\n",
    "- **env.step(action):** \n",
    "    Applies an action to it. It outputes next state, reward, done and info\n",
    "- **env.close():** \n",
    "    Closes the pop-up frame of the visualized environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2427cbbf",
   "metadata": {},
   "source": [
    "## Main problem: Let's learn an agent that uses Q-learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcade68a",
   "metadata": {},
   "source": [
    "As mentioned above, we will first use implement a classical approach of TD-learning, i.e. Q-learning. Q-learning is an off-policy, tabular approach to reinforcement learning. Hence, we need to discretize the state space before proceeding.\n",
    "\n",
    "In the first step we need to digitize the observation space. Here we want to equally split the space into 19 bins using the function np.digitize. \n",
    "\n",
    "It is important to keep in mind that we do not want to discretize the state space too much, given the **GLIE theorem**. **GLIE theorem stands for 'Greedy in the Limit with Infinite Exploration', which means that the state action pairs Q(s,a) will converge to the optimal ones if all state-action pairs are explored infinitely many times and the policy converges to the greedy one**. For this reason, we want to have not too many states and we need to tune the epsilon rate in such a way that it converges to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36184fe3",
   "metadata": {},
   "source": [
    "**ToDo:** Complete the function below that discretizes the observation space of any environment. Make use of the two functions, np.linspace and np.digitze to bin the observation space into equal bins and to allocate an observation into the corresponding bin!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bff4a6",
   "metadata": {},
   "source": [
    "Before we create a Qagent class, we will write single function that we will use for the class and make sure we understand the core pieces we need. In particular, we will need a function that discretizes the state space and a function that creates a Q table!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d7f34ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretizing_obs_state(env_name, state, bin_size=20):\n",
    "    \n",
    "    '''\n",
    "    Params:\n",
    "    env_name = environment that the agent needs to solve \n",
    "    state = observation of the state the agent is in\n",
    "    bin_size = number of bins used for the tabular approach\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #State incorporates the observation state\n",
    "    #State[0] is x position\n",
    "    #State[1] is y position\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    #env.reset()\n",
    "    \n",
    "    #Get the low and high values of the environment space\n",
    "    low = env.observation_space.low\n",
    "    high =env.observation_space.high\n",
    "    \n",
    "    '''\n",
    "    ToDo: Make use of np.linspace to create bins of equal size for the x posiion and the velocity!\n",
    "    Call the bins bin_x and bin_velocity respectively\n",
    "    '''\n",
    "    \n",
    "    #Solution:\n",
    "    bin_x = np.linspace(low[0], high[0], bin_size)\n",
    "    bin_velocity = np.linspace(low[1], high[1], bin_size)\n",
    "    \n",
    "    #Append the two bins\n",
    "    bins = [bin_x, bin_velocity]\n",
    "    \n",
    "    #Now we can make use of the function np.digitize and bin it\n",
    "    digitized_state = []\n",
    "     \n",
    "    for i in range(len(bins)):\n",
    "        digitized_state.append(np.digitize(state[i], bins[i])-1)\n",
    "    \n",
    "    return digitized_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efb0ed1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Q_table(bin_size, action_space):\n",
    "    '''\n",
    "    Params:\n",
    "    bin_size = number of bins used\n",
    "    action_space = number of discrete actions\n",
    "    \n",
    "    Returns:\n",
    "    Matrix of dimension state_space * state_space * action_space with zero entries\n",
    "    '''\n",
    "    \n",
    "    state_space = bin_size - 1\n",
    "    \n",
    "    '''\n",
    "    ToDo: \n",
    "    Return the Q table that has the right dimensions and is filled with zeroes!\n",
    "    '''\n",
    "    \n",
    "    #Solution:\n",
    "    \n",
    "    return np.zeros((state_space, state_space, action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92768c6d",
   "metadata": {},
   "source": [
    "## Now we can put it together and work out a Q learning agent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e020be",
   "metadata": {},
   "source": [
    "In the final step, we can put everything together and create QAgent that learns how to solve the mountain car environment. An important aspect is the policy with which the agent chooses the action. A common choice is epsilon greedy, which means that with a probability fo $\\epsilon$, the agent picks a random action, and with $1-\\epsilon$ the agent picks the action that maximizes the corresponding Q value!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16655903",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent():\n",
    "    \n",
    "    def __init__(self, env_name, discount_rate = 0.95, bin_size = 20):\n",
    "        \n",
    "        '''\n",
    "        Params:\n",
    "        \n",
    "        env_name = name of the specific environment that the agent wants to solve\n",
    "        discount_rate = discount rate used for future rewards\n",
    "        bin_size = number of bins used for discretizing the state space\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #create an environment\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(env_name)\n",
    "        \n",
    "        #Set the discount rate\n",
    "        self.discount_rate = discount_rate\n",
    "        \n",
    "        #The algorithm has then 3 different actions\n",
    "        #0: Accelerate to the left\n",
    "        #1: Don't accelerate\n",
    "        #2: Accelerate to the right\n",
    "        self.action_space = range(env.action_space.n)\n",
    "        \n",
    "        #Set the bin size\n",
    "        self.bin_size = bin_size\n",
    "        \n",
    "        #State incorporates the observation state\n",
    "        #State[0] is x position\n",
    "        #State[1] is y position\n",
    "    \n",
    "        #Get the low and high values of the environment space\n",
    "        self.low = self.env.observation_space.low\n",
    "        self.high = self.env.observation_space.high\n",
    "    \n",
    "        #You could play around with the bin size!\n",
    "        self.bin_x = np.linspace(self.low[0], self.high[0], self.bin_size)\n",
    "        self.bin_velocity = np.linspace(self.low[1], self.high[1], self.bin_size)\n",
    "    \n",
    "        #Append the two bins\n",
    "        self.bins = [self.bin_x, self.bin_velocity]\n",
    "    \n",
    "    def discretize_state(self, state):\n",
    "        \n",
    "        '''\n",
    "        Params:\n",
    "        state = state observation that needs to be discretized\n",
    "        \n",
    "        \n",
    "        Returns:\n",
    "        discretized state\n",
    "        '''\n",
    "        #Now we can make use of the function np.digitize and bin it\n",
    "        self.state = state\n",
    "        \n",
    "        #Create an empty state\n",
    "        digitized_state = []\n",
    "    \n",
    "        for i in range(len(self.bins)):\n",
    "            digitized_state.append(np.digitize(self.state[i], self.bins[i])-1)\n",
    "        \n",
    "        #Returns the discretized state from an observation\n",
    "        return digitized_state\n",
    "    \n",
    "    def create_Q_table(self):\n",
    "        self.state_space = self.bin_size - 1\n",
    "        #Initialize all values in the Q-table to zero\n",
    "        self.Qtable = np.zeros((self.state_space, self.state_space, self.env.action_space.n))\n",
    "\n",
    "    def train(self, simulations, learning_rate, epsilon = 1, adapting_learning_rate = False):\n",
    "        \n",
    "        '''\n",
    "        Params:\n",
    "        \n",
    "        simulations = number of episodes of a game to run\n",
    "        learning_rate = learning rate for the update eqau\n",
    "        epsilon = epsilon value for epsilon-greedy algorithm\n",
    "        adapting_learning_rate = boolean that indicates if the learning rate should be adaptive or not\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Initialize variables that keep track of the rewards\n",
    "        \n",
    "        self.rewards = []\n",
    "        self.average_rewards = []\n",
    "        \n",
    "        #Call the Q table function to create a randomly initialized Q table\n",
    "        self.create_Q_table()\n",
    "        \n",
    "        #Set epsilon rate and learning rate\n",
    "        self.epsilon = epsilon\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        #Set minimum epsilon, so here we want a minimum exploration rate of 0.05\n",
    "        self.epsilon_minimum = 0.05\n",
    "        \n",
    "        #If we choose adaptive learning rate, we start with a value of 1 and decay it over time!\n",
    "        if adapting_learning_rate:\n",
    "            self.learning_rate = 1\n",
    "        \n",
    "        for i in range(simulations):\n",
    "            \n",
    "            if i % 5000 == 0:\n",
    "                print(f'Please wait, the algorithm is learning! The current simulation is {i}')\n",
    "            #Initialize the state\n",
    "            state = self.env.reset()\n",
    "        \n",
    "            #Set a variable that flags if an episode has terminated\n",
    "            done = False\n",
    "        \n",
    "            #Discretize the state space\n",
    "            \n",
    "            state = self.discretize_state(state)\n",
    "            \n",
    "            #Set the rewards to 0\n",
    "            total_rewards = 0\n",
    "            \n",
    "            #Loop until an episode has terminated\n",
    "            while not done:\n",
    "                \n",
    "                #Pick an action based on epsilon greedy\n",
    "                \n",
    "                '''\n",
    "                ToDo: Write a if-else statement that decides based on epsilon greedy which action to take!\n",
    "                Tip: Make use of np.random.uniform() and the self.epsilon to make a decision!\n",
    "                '''\n",
    "                \n",
    "                #Solution:\n",
    "                \n",
    "                #Pick random action\n",
    "                if np.random.uniform(0,1) > 1-self.epsilon:\n",
    "                    #This picks a random action from 0,1,2\n",
    "                    action = np.random.choice(self.action_space)\n",
    "                    \n",
    "                #Pick a greedy action\n",
    "                else:\n",
    "                    action = np.argmax(self.Qtable[state[0],state[1],:])\n",
    "                    \n",
    "                #Now sample the next_state, reward, done and info from the environment\n",
    "                \n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                \n",
    "                #Now discretize the next_state\n",
    "                next_state = self.discretize_state(next_state)\n",
    "                \n",
    "                #Target value \n",
    "                Q_target = (reward + self.discount_rate*np.max(self.Qtable[next_state[0], next_state[1]]))\n",
    "                \n",
    "                #Calculate the Temporal difference error (delta)\n",
    "                delta = self.learning_rate * (Q_target - self.Qtable[state[0], state[1], action])\n",
    "                \n",
    "                #Update the Q-value\n",
    "                self.Qtable[state[0], state[1], action] = self.Qtable[state[0], state[1], action] + delta\n",
    "                \n",
    "                #Update the reward and the hyperparameters\n",
    "                total_rewards += reward\n",
    "                state = next_state\n",
    "                \n",
    "            #Decay epsilon, here we decay both the epsilon and learning rate with the number of simulations\n",
    "            delta = (self.epsilon - self.epsilon_minimum)/(i+1)\n",
    "            self.epsilon -= delta\n",
    "            \n",
    "            if adapting_learning_rate:\n",
    "                self.learning_rate = self.learning_rate/np.sqrt(i+1)\n",
    "            \n",
    "            self.rewards.append(total_rewards)\n",
    "            \n",
    "            #Calculate the average score over 100 episodes\n",
    "            if i % 100 == 0:\n",
    "                self.average_rewards.append(np.mean(self.rewards))\n",
    "                \n",
    "                #Initialize a new reward list, as otherwise the average values would reflect all rewards!\n",
    "                self.rewards = []\n",
    "        \n",
    "        print('The simulation is done!')\n",
    "        \n",
    "    def visualize_rewards(self):\n",
    "        plt.figure(figsize =(7.5,7.5))\n",
    "        plt.plot(100*(np.arange(len(self.average_rewards))+1), self.average_rewards)\n",
    "        plt.axhline(y = -110, color = 'r', linestyle = '-')\n",
    "        plt.title('Average reward over the past 100 simulations', fontsize = 10)\n",
    "        plt.legend(['Q-learning performance','Benchmark'])\n",
    "        plt.xlabel('Number of simulations', fontsize = 10)\n",
    "        plt.ylabel('Average reward', fontsize = 10)\n",
    "            \n",
    "    def play_game(self):\n",
    "        #Get the optimized strategy:\n",
    "        done = False\n",
    "        #Start the game\n",
    "        state = self.env.reset()\n",
    "        while not done:\n",
    "            state = self.discretize_state(state)\n",
    "            #Pick the best acction from the saved Qtable\n",
    "            action = np.argmax(self.Qtable[state[0],state[1],:])\n",
    "            next_state, reward, done, info = self.env.step(action)\n",
    "            state = next_state\n",
    "            self.env.render()\n",
    "            #Pause to make it easier to watch\n",
    "            time.sleep(0.05)\n",
    "        #Close the pop-up window\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3223f42b",
   "metadata": {},
   "source": [
    "## Let's run this simulation let's say 15,000 times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec55c60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait, the algorithm is learning! The current simulation is 0\n",
      "Please wait, the algorithm is learning! The current simulation is 5000\n",
      "Please wait, the algorithm is learning! The current simulation is 10000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "ToDo:\n",
    "Initialize the Qagent and and let it train over 15,000 episodes with a learning rate of your choice! \n",
    "'''\n",
    "\n",
    "#Solution:\n",
    "Qagent = QAgent(env_name, bin_size = 20)\n",
    "Qagent.train(15000, learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27573627",
   "metadata": {},
   "source": [
    "## Now we can plot the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0838f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qagent.visualize_rewards()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca797351",
   "metadata": {},
   "source": [
    "Even though the TD algorithm improves over time, it fails to completely solve the environment, as it does not reach the benchmark of reaching an average reward of -110 ([check this link for further info](https://github.com/openai/gym/wiki/Leaderboard#mountaincar-v0). Yet, we can check if the algorithm at least manages to the drive up the hill (see next section)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0acf56",
   "metadata": {},
   "source": [
    "## Lastly, and finally we want to see if our QAgent learned the problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc0e112",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qagent.play_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0707f32",
   "metadata": {},
   "source": [
    "**Done! Did your agent managed to climb up the hill? ;)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2522147",
   "metadata": {},
   "source": [
    "## Further extensions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba9375d",
   "metadata": {},
   "source": [
    "The code above involves the standard tabular Q learning approach. Yet, one can extend this framework, by for example, \n",
    "make a warm start that, i.e. set the starting position different from the bottom of the hill, to help the agent explore the state space better. Yet, one can consider this as a form of cheating. \n",
    "\n",
    "**In the second notebook, we will see how we can extend the framework by approximating the Q function with a neural network, i.e. by introducing deep reinforcement learning!**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
