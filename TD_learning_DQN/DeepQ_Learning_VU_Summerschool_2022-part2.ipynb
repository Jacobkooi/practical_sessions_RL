{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7549b86",
   "metadata": {},
   "source": [
    "# Reinforcement learning summer school at the VU - 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb46de00",
   "metadata": {},
   "source": [
    "## Workshop tutorial, day 2: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55dd0d8",
   "metadata": {},
   "source": [
    "## Deep Reinforcement Learning Agent (Part 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5dc2ee",
   "metadata": {},
   "source": [
    "### Author: Buelent Uendes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879505c9",
   "metadata": {},
   "source": [
    "In this notebook, we will use a function approximator to solve the mountain car game. As seen in the previous notebook, a simple agent that uses Q learning can learn to move the car in a way to move up the hill. Yet, for this to work, one had to discretize the state space. However, for large problems this approach is not feasible, given the fact that we then have a state,action pair matrix. To overcome this, we will use a Neural Network that will approximate the state, pair values. For this, we will use PyTorch. If you have not used PyTorch yet, do not worry, as the code will be provided. Yet, if you want to have a more in-depth tutorial in PyTorch, you can use the following YouTube tutorial:\n",
    "\n",
    "- https://www.youtube.com/watch?v=c36lUUr864M\n",
    "\n",
    "Deep reinforcement learning got popular following the paper published in 2013 [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf). Following this paper, several additional techniques were introduced that aim to stabilize the learning process. In the following notebook, we will look at two new methods, memory replay and target networks and will try to solve the mountain car environment using a Deep Reinforcement Learning algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845ef6ad",
   "metadata": {},
   "source": [
    "This notebook is inspired by the implementation of a Deep Q agent as discussed in this [tutorial](https://www.youtube.com/watch?v=NP8pXZdU-5U)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1b5871",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "\n",
    "In the notebook, you will see a couple of ToDos with some instructions. Try your best to work through them and to complete the notebook. In case you run into problems, do not hesitate to ask any of the TAs for help! :) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5bd33c",
   "metadata": {},
   "source": [
    "## Preliminaries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c663abf",
   "metadata": {},
   "source": [
    "### Import main libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa54dd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import deque\n",
    "from mpl_toolkits import mplot3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e0c7d9",
   "metadata": {},
   "source": [
    "### Seeting the seed for reproducibility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fa4924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "seed = 10\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592b8108",
   "metadata": {},
   "source": [
    "## General notes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1f4fab",
   "metadata": {},
   "source": [
    "We will introduce the concept of Deep Reinforcement Learning in **three** steps:\n",
    "\n",
    "1) First introduce how to implement a simple deep neural network that is represents an essential building block of Deep Q learning\n",
    "\n",
    "2) Introduce the topic of experience replay/replay buffer\n",
    "\n",
    "3) Introduce the concept of a target network!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f29625",
   "metadata": {},
   "source": [
    "## Part 1: Deep neural network and its general characteristics in the context of reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01122b51",
   "metadata": {},
   "source": [
    "### General characteristics of Deep Q learning\n",
    "\n",
    "In the simplest approach, a Deep RL algorithm is:\n",
    "\n",
    "- Episodic (the agent acts in the environment only for a specific number of timesteps)\n",
    "- Online (we train the algorithm while the agent interacts with the environment)\n",
    "- Model-free. We do not attempt to model the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecc3b72",
   "metadata": {},
   "source": [
    "In the following, we will implement a deep neural network using the PyTorch library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e48f525",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, env, learning_rate):\n",
    "        \n",
    "        super(DQN,self).__init__()\n",
    "        \n",
    "        '''\n",
    "        ToDo: Write your code here! \n",
    "        Make sure that the input features and the output features are in line with the environment that \n",
    "        the class takes as an input feature\n",
    "        '''\n",
    "        \n",
    "        'Solution:'\n",
    "        input_features = env.observation_space.shape[0]\n",
    "        action_space = env.action_space.n\n",
    "        \n",
    "        self.dense1 = nn.Linear(in_features = input_features, out_features = 128)\n",
    "        self.dense2 = nn.Linear(in_features = 128, out_features = 64)\n",
    "        self.dense3 = nn.Linear(in_features = 64, out_features = 32)\n",
    "        self.dense4 = nn.Linear(in_features = 32, out_features = action_space)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        '''\n",
    "        ToDo: Write the forward pass! Keep in mind that the final layer should have as\n",
    "        many output features as there are actions!\n",
    "        \n",
    "        Important: We want to output a linear activation function as we need the q-values associated with each action\n",
    "    \n",
    "        '''\n",
    "        \n",
    "        'Solution:'\n",
    "        x = torch.tanh(self.dense1(x))\n",
    "        x = torch.tanh(self.dense2(x))\n",
    "        x = torch.tanh(self.dense3(x))\n",
    "        x = self.dense4(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32692ba5",
   "metadata": {},
   "source": [
    "That's it! This is the implementation of a deep neural network in PyTorch!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cc5db9",
   "metadata": {},
   "source": [
    "## Part 2: Experience replay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618bcd95",
   "metadata": {},
   "source": [
    "In a normal implementation of a deep neural network, one would train the algorithm using some sort of a gradient method. Yet, one of the key assumption is that the data is iid, i.e. independent identically distributed which does not hold in our reinforcement learning setting. The next state and its reward depends on the action our agent took the preceeding state which makes subsequent states and the data highly correlated. This can cause the DQN to be instable. To circumvent this, people use in practice a so-called experience replay technique. The main rationale behind this idea is to break the correlation between subsequent transitions by saving experiences in memory and sample randomly from the stored transitions when performing a Q-value update. This 'trick' is essential to make the method work!\n",
    "\n",
    "In the following, we will create a experience replay class that will store the transitions of the deep Q agent. It is important to keep in mind that the replay buffer has a fixed capacity. If the data that we want to store in the replay buffer exceeds the buffer, we want to store only the most recent transitions in the buffer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f13b6f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    \n",
    "    def __init__(self, env, buffer_size, min_replay_size = 1000):\n",
    "        \n",
    "        self.env = env\n",
    "        self.min_replay_size = min_replay_size\n",
    "        self.replay_buffer = deque(maxlen=buffer_size)\n",
    "        self.reward_buffer = deque([0.0], maxlen = 100)\n",
    "        \n",
    "        print('Please wait, the experience replay buffer will be filled with random transitions')\n",
    "        \n",
    "        '''\n",
    "        ToDo: \n",
    "        Write a for loop that initializes the experience replay buffer with random transitions \n",
    "        such that the experience replay buffer \n",
    "        has minimum random transitions already stored \n",
    "        '''\n",
    "        \n",
    "        'Solution'\n",
    "        obs = self.env.reset()\n",
    "        \n",
    "        #Write your code here!\n",
    "        for _ in range(self.min_replay_size):\n",
    "            \n",
    "            action = env.action_space.sample()\n",
    "            new_obs, rew, done, _ = env.step(action)\n",
    "            transition = (obs, action, rew, done, new_obs)\n",
    "            self.replay_buffer.append(transition)\n",
    "            obs = new_obs\n",
    "    \n",
    "            if done:\n",
    "                obs = env.reset()\n",
    "        \n",
    "        #Logging\n",
    "        print('Initialization with random transitions is done!')\n",
    "      \n",
    "          \n",
    "    def add_data(self, data):      \n",
    "        self.replay_buffer.append(data)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        \n",
    "        transitions = random.sample(self.replay_buffer, batch_size)\n",
    "\n",
    "        observations = np.asarray([t[0] for t in transitions])\n",
    "        actions = np.asarray([t[1] for t in transitions])\n",
    "        rewards = np.asarray([t[2] for t in transitions])\n",
    "        dones = np.asarray([t[3] for t in transitions])\n",
    "        new_observations = np.asarray([t[4] for t in transitions])\n",
    "\n",
    "        observations_t = torch.as_tensor(observations, dtype = torch.float32)\n",
    "        actions_t = torch.as_tensor(actions, dtype = torch.int64).unsqueeze(-1)\n",
    "        rewards_t = torch.as_tensor(rewards, dtype = torch.float32).unsqueeze(-1)\n",
    "        dones_t = torch.as_tensor(dones, dtype = torch.float32).unsqueeze(-1)\n",
    "        new_observations_t = torch.as_tensor(new_observations, dtype = torch.float32)\n",
    "        \n",
    "        return observations_t, actions_t, rewards_t, dones_t, new_observations_t\n",
    "    \n",
    "    def add_reward(self, reward):\n",
    "        self.reward_buffer.append(reward)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e78f29",
   "metadata": {},
   "source": [
    "##  Write policy for choosing the optimal policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58392c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action(env, online_network, observation, step, \n",
    "                          epsilon_decay, epsilon_start, epsilon_end):\n",
    "    \n",
    "    epsilon = np.interp(step, [0, epsilon_decay], [epsilon_start, epsilon_end])\n",
    "    \n",
    "    random_sample = random.random()\n",
    "    \n",
    "    '''\n",
    "    ToDo: write the function here that returns an action that is chosen based on the epsilon greedy policy\n",
    "    '''\n",
    "    \n",
    "    'Solution:'\n",
    "    \n",
    "    #Random action\n",
    "    if random_sample <= epsilon:\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "    #Greedy action\n",
    "    else:\n",
    "        obs_t = torch.as_tensor(observation, dtype = torch.float32)\n",
    "        q_values = self.net(obs_t.unsqueeze(0))\n",
    "        \n",
    "        max_q_index = torch.argmax(q_values, dim = 1)[0]\n",
    "        action = max_q_index.detach().item()\n",
    "        \n",
    "    return action, epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a756de",
   "metadata": {},
   "source": [
    "## Write the code for the vanilla DQN agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29b36a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class vanilla_DQNAgent:\n",
    "    \n",
    "    def __init__(self, en, device, epsilon_decay, \n",
    "                 epsilon_start, epsilon_end, discount_rate, lr, buffer_size):\n",
    "        \n",
    "        self.env = env\n",
    "        self.discount_rate = discount_rate\n",
    "        self.device = device\n",
    "        \n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        \n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = lr\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "        self.replay_memory = ExperienceReplay(self.env, self.buffer_size)\n",
    "        self.online_network = DQN(self.env, self.learning_rate).to(self.device)\n",
    "        \n",
    "    def choose_action(self, step, observation, greedy = False):\n",
    "        \n",
    "        '''\n",
    "        Copy paste your epsilon greedy function that you wrote above!\n",
    "        '''\n",
    "        \n",
    "        epsilon = np.interp(step, [0, self.epsilon_decay], [self.epsilon_start, self.epsilon_end])\n",
    "    \n",
    "        random_sample = random.random()\n",
    "    \n",
    "        if (random_sample <= epsilon) and not greedy:\n",
    "            #Random action\n",
    "            action = self.env.action_space.sample()\n",
    "        \n",
    "        else:\n",
    "            obs_t = torch.as_tensor(observation, dtype = torch.float32)\n",
    "            q_values = self.online_network(obs_t.unsqueeze(0))\n",
    "        \n",
    "            max_q_index = torch.argmax(q_values, dim = 1)[0]\n",
    "            action = max_q_index.detach().item()\n",
    "        \n",
    "        return action, epsilon\n",
    "    \n",
    "    def learn(self, batch_size):\n",
    "        \n",
    "        observations_t, actions_t, rewards_t, dones_t, new_observations_t = self.replay_memory.sample(batch_size)\n",
    "\n",
    "        #Compute targets, note that we use the same neural network to do both! This will be changed later!\n",
    "        \n",
    "        '''ToDo:\n",
    "        Calculate the target q values and the corresponding max q values!\n",
    "        '''\n",
    "        \n",
    "        'Solution:'\n",
    "        target_q_values = self.online_network(new_observations_t)\n",
    "        max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]\n",
    "\n",
    "        targets = rewards_t + self.discount_rate * (1-dones_t) * max_target_q_values\n",
    "\n",
    "        #Compute loss\n",
    "\n",
    "        q_values = self.online_network(observations_t)\n",
    "\n",
    "        action_q_values = torch.gather(input=q_values, dim=1, index=actions_t)\n",
    "\n",
    "        #Loss, here we take the huber loss!\n",
    "        \n",
    "        'ToDo: Implement here the loss function! You can choose the standard MSE loss or Huber loss'\n",
    "\n",
    "        'Solution:'\n",
    "        loss = F.smooth_l1_loss(action_q_values, targets)\n",
    "        \n",
    "        #loss = F.mse_loss(action_q_values, targets)\n",
    "        \n",
    "        'ToDo: Write the gradient descent step, were you optimize the online network based on the loss!'\n",
    "        \n",
    "        'Solution:'\n",
    "        #Gradient descent\n",
    "        self.online_network.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.online_network.optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eceee3",
   "metadata": {},
   "source": [
    "## Write the training loop and perform the first run!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e78d68",
   "metadata": {},
   "source": [
    "In a last step, we can write a training loop that will put all things together. We will run the training loop for a number of iteration and see how our first algorithm performs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cae3b0",
   "metadata": {},
   "source": [
    "### Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40b0f49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the hyperparameters\n",
    "\n",
    "#Discount rate\n",
    "discount_rate = 0.99\n",
    "#That is the sample that we consider to update our algorithm\n",
    "batch_size = 32\n",
    "#Maximum number of transitions that we store in the buffer\n",
    "buffer_size = 50000\n",
    "\n",
    "min_replay_size = 1000\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.05\n",
    "#Decay period until epsilon start -> epsilon end\n",
    "epsilon_decay = 10000\n",
    "\n",
    "max_episodes = 250000\n",
    "\n",
    "lr = 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc06312",
   "metadata": {},
   "source": [
    "### Set the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb88496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if this works\n",
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d947bf",
   "metadata": {},
   "source": [
    "### Initialize all instances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecf59524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait, the experience replay buffer will be filled with random transitions\n",
      "Initialization with random transitions is done!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vanilla_agent = vanilla_DQNAgent(env, device, epsilon_decay, epsilon_start, epsilon_end, discount_rate, lr, buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f68caa4",
   "metadata": {},
   "source": [
    "### Main training loop - vanilla DQN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b76e9bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Step 0\n",
      "Epsilon 1.0\n",
      "Avg Rew 0.0\n",
      "\n",
      "----------------------------------------\n",
      "Step 10000\n",
      "Epsilon 0.05\n",
      "Avg Rew -196.07843137254903\n",
      "\n",
      "----------------------------------------\n",
      "Step 20000\n",
      "Epsilon 0.05\n",
      "Avg Rew -200.0\n",
      "\n",
      "----------------------------------------\n",
      "Step 30000\n",
      "Epsilon 0.05\n",
      "Avg Rew -200.0\n",
      "\n",
      "----------------------------------------\n",
      "Step 40000\n",
      "Epsilon 0.05\n",
      "Avg Rew -200.0\n",
      "\n",
      "----------------------------------------\n",
      "Step 50000\n",
      "Epsilon 0.05\n",
      "Avg Rew -200.0\n",
      "\n",
      "----------------------------------------\n",
      "Step 60000\n",
      "Epsilon 0.05\n",
      "Avg Rew -200.0\n",
      "\n",
      "----------------------------------------\n",
      "Step 70000\n",
      "Epsilon 0.05\n",
      "Avg Rew -200.0\n",
      "\n",
      "----------------------------------------\n",
      "Step 80000\n",
      "Epsilon 0.05\n",
      "Avg Rew -200.0\n",
      "\n",
      "----------------------------------------\n",
      "Step 90000\n",
      "Epsilon 0.05\n",
      "Avg Rew -200.0\n",
      "\n",
      "----------------------------------------\n",
      "Step 100000\n",
      "Epsilon 0.05\n",
      "Avg Rew -200.0\n",
      "\n",
      "----------------------------------------\n",
      "Step 110000\n",
      "Epsilon 0.05\n",
      "Avg Rew -200.0\n",
      "\n",
      "----------------------------------------\n",
      "Step 120000\n",
      "Epsilon 0.05\n",
      "Avg Rew -200.0\n",
      "\n",
      "----------------------------------------\n",
      "Step 130000\n",
      "Epsilon 0.05\n",
      "Avg Rew -200.0\n",
      "\n",
      "----------------------------------------\n",
      "Step 140000\n",
      "Epsilon 0.05\n",
      "Avg Rew -200.0\n",
      "\n",
      "----------------------------------------\n",
      "Step 150000\n",
      "Epsilon 0.05\n",
      "Avg Rew -200.0\n",
      "\n",
      "----------------------------------------\n",
      "Step 160000\n",
      "Epsilon 0.05\n",
      "Avg Rew -200.0\n",
      "\n",
      "----------------------------------------\n",
      "Step 170000\n",
      "Epsilon 0.05\n",
      "Avg Rew -200.0\n",
      "\n",
      "----------------------------------------\n",
      "Step 180000\n",
      "Epsilon 0.05\n",
      "Avg Rew -200.0\n",
      "\n",
      "----------------------------------------\n",
      "Step 190000\n",
      "Epsilon 0.05\n",
      "Avg Rew -200.0\n",
      "\n",
      "----------------------------------------\n",
      "Step 200000\n",
      "Epsilon 0.05\n",
      "Avg Rew -200.0\n",
      "\n",
      "----------------------------------------\n",
      "Step 210000\n",
      "Epsilon 0.05\n",
      "Avg Rew -200.0\n",
      "\n",
      "----------------------------------------\n",
      "Step 220000\n",
      "Epsilon 0.05\n",
      "Avg Rew -200.0\n",
      "\n",
      "----------------------------------------\n",
      "Step 230000\n",
      "Epsilon 0.05\n",
      "Avg Rew -200.0\n",
      "\n",
      "----------------------------------------\n",
      "Step 240000\n",
      "Epsilon 0.05\n",
      "Avg Rew -200.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Main training loop\n",
    "env.action_space.seed(1)\n",
    "obs = env.reset()\n",
    "#Have a list that stores average reward over 100 runs\n",
    "average_rewards_vanilla_dqn = [0]\n",
    "episode_reward = 0.0\n",
    "\n",
    "for step in range(max_episodes):\n",
    "    \n",
    "    action, epsilon = vanilla_agent.choose_action(step, obs)\n",
    "       \n",
    "    new_obs, rew, done, _ = env.step(action)\n",
    "    transition = (obs, action, rew, done, new_obs)\n",
    "    vanilla_agent.replay_memory.add_data(transition)\n",
    "    obs = new_obs\n",
    "    \n",
    "    episode_reward += rew\n",
    "    \n",
    "    if done:\n",
    "        \n",
    "        obs = env.reset()\n",
    "        vanilla_agent.replay_memory.add_reward(episode_reward)\n",
    "        #Reinitilize the reward to 0.0 after the game is over\n",
    "        episode_reward = 0.0\n",
    "\n",
    "#Learn\n",
    "\n",
    "    vanilla_agent.learn(batch_size)\n",
    "\n",
    "    #Calculate after each 100 episodes an average that will be added to the list\n",
    "    \n",
    "    if (step+1) % 100 == 0:\n",
    "        average_rewards_vanilla_dqn.append(np.mean(vanilla_agent.replay_memory.reward_buffer))\n",
    "    \n",
    "    #Logging\n",
    "    if step % 10000 == 0:\n",
    "        print(20*'--')\n",
    "        print('Step', step)\n",
    "        print('Epsilon', epsilon)\n",
    "        print('Avg Rew', np.mean(vanilla_agent.replay_memory.reward_buffer))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743ad14c",
   "metadata": {},
   "source": [
    "**Comment**: As you can see, the vanilla deep Q network performs very poorly and does not learn to master the challenge. Play around with the number of iterations and epsilon decay to check if you can improve the algorithm!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd015ad5",
   "metadata": {},
   "source": [
    "## Part 3: Target network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a9db3c",
   "metadata": {},
   "source": [
    "A problem of the standard Q learning introduced above is the fact that we use the same Q value to choose an action and to evaluate it. To overcome this problem, double-Q learning was proposed in the following paper [Double Q-learning](https://papers.nips.cc/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf).\n",
    "In the case of DQN, we can make use of the same idea by training a second neural network,a so-called target network. Just as the name suggests, the target network will be used to compute the target of the update equation using this target network. This target network will only be updated after a pre-defined number of steps to ensure that the target will not move as the DQN network will learn (as it is the case in the standard simple DQN framework). This idea was put forward in the paper again by van Hasselt et al. (2016) [Deep reinforcement learning with double Q-learning](https://arxiv.org/pdf/1509.06461.pdf). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2407fc05",
   "metadata": {},
   "source": [
    "Implementing a target network and changing the architecture to a double DQN is rather straightforward. All we need to do is to initialize besides an online network a so-called target network. After a specific number of steps, the parameter values of the target network are reinitialized with the online network after a pre-defined number of steps. For this, we will add a few lines to the vanilla DQN class and call it DDQN (for double deep Q learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d870289",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNAgent:\n",
    "    \n",
    "    def __init__(self, en, device, epsilon_decay, \n",
    "                 epsilon_start, epsilon_end, discount_rate, lr, buffer_size):\n",
    "        \n",
    "        self.env = env\n",
    "        self.discount_rate = discount_rate\n",
    "        self.device = device\n",
    "        \n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        \n",
    "        self.learning_rate = lr\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "        self.replay_memory = ExperienceReplay(self.env, self.buffer_size)\n",
    "        self.online_network = DQN(self.env, self.learning_rate).to(self.device)\n",
    "        \n",
    "        '''\n",
    "        ToDo: 'Add here a target network and set the parameter values to the ones of the online network!'\n",
    "        '''\n",
    "        \n",
    "        'Solution'\n",
    "        self.target_network = DQN(self.env, self.learning_rate).to(self.device)\n",
    "        self.target_network.load_state_dict(self.online_network.state_dict())\n",
    "        \n",
    "    def choose_action(self, step, observation, greedy = False):\n",
    "        \n",
    "        '''\n",
    "        Copy paste here the code from above!\n",
    "        '''\n",
    "        \n",
    "        epsilon = np.interp(step, [0, self.epsilon_decay], [self.epsilon_start, self.epsilon_end])\n",
    "    \n",
    "        random_sample = random.random()\n",
    "    \n",
    "        if (random_sample <= epsilon) and not greedy:\n",
    "            #Random action\n",
    "            action = self.env.action_space.sample()\n",
    "        \n",
    "        else:\n",
    "            obs_t = torch.as_tensor(observation, dtype = torch.float32)\n",
    "            q_values = self.online_network(obs_t.unsqueeze(0))\n",
    "        \n",
    "            max_q_index = torch.argmax(q_values, dim = 1)[0]\n",
    "            action = max_q_index.detach().item()\n",
    "        \n",
    "        return action, epsilon\n",
    "    \n",
    "    #We will need this function later for plotting the 3D graph\n",
    "    def return_q_value(self, observation):\n",
    "        obs_t = torch.as_tensor(observation, dtype = torch.float32)\n",
    "        q_values = self.online_network(obs_t.unsqueeze(0))\n",
    "        \n",
    "        return torch.max(q_values).item()\n",
    "        \n",
    "    def learn(self, batch_size):\n",
    "        \n",
    "        observations_t, actions_t, rewards_t, dones_t, new_observations_t = self.replay_memory.sample(batch_size)\n",
    "\n",
    "        #Compute targets, note that we use the same neural network to do both! This will be changed later!\n",
    "\n",
    "        target_q_values = self.target_network(new_observations_t)\n",
    "        max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]\n",
    "\n",
    "        targets = rewards_t + self.discount_rate * (1-dones_t) * max_target_q_values\n",
    "\n",
    "        #Compute loss\n",
    "\n",
    "        q_values = self.online_network(observations_t)\n",
    "\n",
    "        action_q_values = torch.gather(input=q_values, dim=1, index=actions_t)\n",
    "\n",
    "        #Loss, here we take the huber loss!\n",
    "\n",
    "        loss = F.smooth_l1_loss(action_q_values, targets)\n",
    "        \n",
    "        #loss = F.mse_loss(action_q_values, targets)\n",
    "        \n",
    "        #Gradient descent\n",
    "        self.online_network.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.online_network.optimizer.step()\n",
    "        \n",
    "    '''\n",
    "    ToDO: Write a method 'update_target_network(self), \n",
    "    which updats the target network with the parameters of the online network\n",
    "    '''\n",
    "    \n",
    "    'Solution'\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.online_network.state_dict())\n",
    "    \n",
    "    #This method will let the DQNAgent play the game after it has worked through the number of episodes for training\n",
    "    \n",
    "    def play_game(self, step):\n",
    "        #Get the optimized strategy:\n",
    "        done = False\n",
    "        #Start the game\n",
    "        state = self.env.reset()\n",
    "        while not done:\n",
    "            #Pick the best acction from the saved Qtable\n",
    "            action = self.choose_action(step, state, True)[0]\n",
    "            next_state, reward, done, info = self.env.step(action)\n",
    "            state = next_state\n",
    "            self.env.render()\n",
    "            #Pause to make it easier to watch\n",
    "            time.sleep(0.05)\n",
    "        #Close the pop-up window\n",
    "        self.env.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e02d62e",
   "metadata": {},
   "source": [
    "After we have created our DDQNAgent class, we can re-run the experiment from above and see if we can increase the performance! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba258ff0",
   "metadata": {},
   "source": [
    "## Hyperparameters and initialization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bb7d7f",
   "metadata": {},
   "source": [
    "Since the hyperparameters are the same as before, we only need to set the new hyperparameter target_update_frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23699824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the new hyperparameters\n",
    "target_update_frequency = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89799b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait, the experience replay buffer will be filled with random transitions\n",
      "Initialization with random transitions is done!\n"
     ]
    }
   ],
   "source": [
    "dagent = DDQNAgent(env, device, epsilon_decay, epsilon_start, epsilon_end, discount_rate, lr, buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef28616",
   "metadata": {},
   "source": [
    "### Main loop DDQN - double deep Q network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89e6f851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Step 0\n",
      "Epsilon 1.0\n",
      "Avg Rew 0.0\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Step 10000\n",
      "Epsilon 0.05\n",
      "Avg Rew -196.07843137254903\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Step 20000\n",
      "Epsilon 0.05\n",
      "Avg Rew -198.69\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Step 30000\n",
      "Epsilon 0.05\n",
      "Avg Rew -198.21\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Step 40000\n",
      "Epsilon 0.05\n",
      "Avg Rew -199.52\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Step 50000\n",
      "Epsilon 0.05\n",
      "Avg Rew -200.0\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Step 60000\n",
      "Epsilon 0.05\n",
      "Avg Rew -198.7\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Step 70000\n",
      "Epsilon 0.05\n",
      "Avg Rew -197.55\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Step 80000\n",
      "Epsilon 0.05\n",
      "Avg Rew -168.58\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Step 90000\n",
      "Epsilon 0.05\n",
      "Avg Rew -176.67\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Step 100000\n",
      "Epsilon 0.05\n",
      "Avg Rew -197.43\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Step 110000\n",
      "Epsilon 0.05\n",
      "Avg Rew -198.99\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Step 120000\n",
      "Epsilon 0.05\n",
      "Avg Rew -191.74\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Step 130000\n",
      "Epsilon 0.05\n",
      "Avg Rew -190.11\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Step 140000\n",
      "Epsilon 0.05\n",
      "Avg Rew -197.2\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Step 150000\n",
      "Epsilon 0.05\n",
      "Avg Rew -197.7\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Step 160000\n",
      "Epsilon 0.05\n",
      "Avg Rew -195.32\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Step 170000\n",
      "Epsilon 0.05\n",
      "Avg Rew -183.34\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Step 180000\n",
      "Epsilon 0.05\n",
      "Avg Rew -141.42\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Step 190000\n",
      "Epsilon 0.05\n",
      "Avg Rew -161.82\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Step 200000\n",
      "Epsilon 0.05\n",
      "Avg Rew -149.47\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Step 210000\n",
      "Epsilon 0.05\n",
      "Avg Rew -142.07\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Step 220000\n",
      "Epsilon 0.05\n",
      "Avg Rew -162.86\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Step 230000\n",
      "Epsilon 0.05\n",
      "Avg Rew -147.59\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Step 240000\n",
      "Epsilon 0.05\n",
      "Avg Rew -150.81\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Main training loop\n",
    "env.action_space.seed(42)\n",
    "obs = env.reset()\n",
    "#Have a list that stores average reward over 100 runs\n",
    "average_rewards_ddqn = [0]\n",
    "episode_reward = 0.0\n",
    "\n",
    "for step in range(max_episodes):\n",
    "    \n",
    "    action, epsilon = dagent.choose_action(step, obs)\n",
    "       \n",
    "    new_obs, rew, done, _ = env.step(action)\n",
    "    transition = (obs, action, rew, done, new_obs)\n",
    "    dagent.replay_memory.add_data(transition)\n",
    "    obs = new_obs\n",
    "    \n",
    "    episode_reward += rew\n",
    "    \n",
    "    if done:\n",
    "        \n",
    "        obs = env.reset()\n",
    "        dagent.replay_memory.add_reward(episode_reward)\n",
    "        #Reinitilize the reward to 0.0 after the game is over\n",
    "        episode_reward = 0.0\n",
    "\n",
    "#Learn\n",
    "\n",
    "    dagent.learn(batch_size)\n",
    "\n",
    "#Calculate after each 100 episodes an average that will be added to the list\n",
    "    \n",
    "    if (step+1) % 100 == 0:\n",
    "        average_rewards_ddqn.append(np.mean(dagent.replay_memory.reward_buffer))\n",
    "    \n",
    "    \n",
    "#Update target network\n",
    "    if step % target_update_frequency == 0:\n",
    "        dagent.update_target_network()\n",
    "    \n",
    "#Logging\n",
    "    if step % 10000 == 0:\n",
    "        print(20*'--')\n",
    "        print('Step', step)\n",
    "        print('Epsilon', epsilon)\n",
    "        print('Avg Rew', np.mean(dagent.replay_memory.reward_buffer))\n",
    "        print(20*'--')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce395b6",
   "metadata": {},
   "source": [
    "**Comments**:\n",
    "\n",
    "As you can see, implementing a target network improved the performance of the deep reinforcement learning algorithm greatly! \n",
    "\n",
    "We can also plot the results of both algorithms to see the difference even more clearly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10958adb",
   "metadata": {},
   "source": [
    "**Comment**:\n",
    "\n",
    "Here we can plot the results of the algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3899c813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Average reward')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(1000*(np.arange(len(average_rewards_ddqn))+1),average_rewards_ddqn)\n",
    "plt.plot(1000*(np.arange(len(average_rewards_vanilla_dqn))+1),average_rewards_vanilla_dqn)\n",
    "plt.title('Average reward over the past 100 simulations')\n",
    "plt.xlabel('Number of simulations')\n",
    "plt.legend(['Double DQN', 'Vanilla DQN'])\n",
    "plt.ylabel('Average reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9efcc22",
   "metadata": {},
   "source": [
    "You can play around the hyperparameter and see how the results change if, for example, you lower the discount rate or learning rate! Also, you can see if changing the neural network architecture, i.e. making it deeper, will lead to an increase in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb70edf1",
   "metadata": {},
   "source": [
    "## Reap the rewards of the hard work - see the DDQN play the game! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e9be68",
   "metadata": {},
   "source": [
    "Now that we worked through two different deep reinforcement learning architectures, we can see the DQN solve the game. The code below with let the DQNAgent play the mountain car game. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c9c9a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "dagent.play_game(max_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164c3580",
   "metadata": {},
   "source": [
    "## Visualize the result in a 3D plot "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244758d1",
   "metadata": {},
   "source": [
    "**ToDo!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1ec157",
   "metadata": {},
   "source": [
    "We can visualize the result in a 3D plot, plotting the x-position as well as the velocity with the corresponding value function. To recall, the value of a particular state is, in case of a greedy policy, the corresponding maximum state action pair! The following function will plot the value function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1214f5",
   "metadata": {},
   "source": [
    "## Extensions/Interesting notes: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0b1527",
   "metadata": {},
   "source": [
    "Following the sucess of the paper by Minh et al. (2013), research in deep reinforcement learning has progressed and a couple of extensions to the basic framework have been proposed, such as dueling deep Q learning (also called D3QN) and priotized experience replay. But before I give you some pointers on this, we will discuss the deadly triad in a bit more detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1ad8ee",
   "metadata": {},
   "source": [
    "## The deadly triad "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605824e3",
   "metadata": {},
   "source": [
    "Some of you might have heard of the term 'deadly triad' which refers to the instability a reinforcement learning algorithm faces, when an algorithm makes use of:\n",
    "\n",
    "- function approximation\n",
    "- bootstrapping\n",
    "- off-policy evaluation\n",
    "\n",
    "Our deep reinforcement learning algorithm makes use of all three concepts. Yet, it does **not** state that instability/divergence always occur when all three above-mentioned techniques are used. The deadly triad only states that it **can** occur. An interesting paper that addresses this issue empirically is the following paper by van Hasselt et al. (2018) [Deep Reinforcement Learning and the Deadly Triad](https://arxiv.org/pdf/1812.02648.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad40ff8",
   "metadata": {},
   "source": [
    "For doing this, they realize that if one bounds the rewards in the interval between $[-1,1]$, then one can show that the corresponding Q values are bounded given by the following equation:\n",
    "\n",
    "$  \\sum_{t'=t}^{T} \\gamma^{t'-t} |r_{t'}| \\le \\sum_{t'=t}^{\\infty} \\gamma^{t'-t} |r_{t'}| \\le \\sum_{t'=t}^{\\infty} \\gamma^{t'-t} = \\frac{1}{1-\\gamma}  $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100cf897",
   "metadata": {},
   "source": [
    "According to this, any Q-value is theoretically bounded by the above equation. In our case, by $100$ (given a discount rate of $0.99$). Hence, if the Q-value exceeds this bound, we say that soft divergence occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bb6b28",
   "metadata": {},
   "source": [
    "## Priotized experience replay "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e293a04",
   "metadata": {},
   "source": [
    "**To be done**\n",
    "\n",
    "Here I will ad a few comments about priotized experience replay.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c1ff58",
   "metadata": {},
   "source": [
    "## Dueling Deep Reinforcement learning -D3QN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcfc00e",
   "metadata": {},
   "source": [
    "**To be done**\n",
    "\n",
    "Here I will add a few lines over D3QN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636a6f95",
   "metadata": {},
   "source": [
    "This tutorial is inspired by Pylessons. You can find the tutorials here [Pylessons](https://pylessons.com/CartPole-DDDQN). The idea of using a dueling deep reinforcement algorithm was introduced by [Wang et al. (2016).](https://arxiv.org/pdf/1511.06581.pdf).\n",
    "\n",
    "[medium](https://markelsanz14.medium.com/introduction-to-reinforcement-learning-part-4-double-dqn-and-dueling-dqn-b349c9a61ea1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
